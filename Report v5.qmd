---
title: "Report"
author: "Justin Powley"
format: html
editor: visual
---

```{r, init, message = F, warning = F, error = F, echo = F}
library(tidyquant)
library(tidyverse)
library(RTL)
library(tsibble)
library(feasts)
library(slider)
library(httr)
library(curl)
library(zoo)
library(forecast)
library(corrplot)
library(broom)
library(car)
library(ggfortify)
library(tseries)
library(kableExtra)

# Note that sometimes the lack of an API sometimes necessitates loading from excel worksheets.
# This is not the equivalent of modeling in excel, it's instead just another mechanism for us to load data automatically into the model.
library(xlsx)

EIA_API <- "UKCNbCB0m8aixfPBshQU3Jdlz2uZsEbG2HebvhHX"
```

```{r, data_import, message = F, warning = F, error = F, echo = F}
# Yahoo Finance
tickers <- c("RB=F", "HO=F", "CL=F", "NG=F")
start_date <- '2018-01-01'
price_data_in <- tq_get(tickers, get = "stock.prices", from = start_date) %>% 
  dplyr::mutate(symbol = case_when(symbol == tickers[1] ~ "RBOB",
                                   symbol == tickers[2] ~ "Heating_Oil",
                                   symbol == tickers[3] ~ "WTI",
                                   symbol == tickers[4] ~ "Natural_Gas"))

eia_tickers <- tribble(~ticker, ~name, "PET.WTTSTUS1.W", "US_Crude_Inventory",
                       "PET.MCRFPUS1.M", "US_Crude_Prod",
                       "NG.NW2_EPG0_SWO_R48_BCF.W", "Lower48NGStorage",
                       "PET.WCRNTUS2.W", "US_Crude_NI",
                       "PET.WRPNTUS2.W", "US_Petrol_Products_NI",
                       "PET.WGFRPUS2.W", "US_Gasoline_Production",
                       "PET.MGFSTUS1.M", "US_Gasoline_Stocks_Blended",
                       "PET.M_EPOBGRR_SAE_NUS_MBBL.M", "US_RBOB_Stocks_Unblended",
                       "PET.MO5ST_NUS_1.M", "US_Conventional_Stocks_Unblended",
                       "PET.WPULEUS3.W", "Pct_Operable_Utilization"
                       )

EIA_Data <- eia2tidy_all(eia_tickers, EIA_API)
# EIA_Data %>% group_by(series) %>% dplyr::mutate(temp = 1, temp = cumsum(temp)) %>% 
#   dplyr::filter(temp == 1) %>% 
#   dplyr::select(-temp)

fred_tickers <- c("IPMINE", "GDP", "TRUCKD11")
fred_data <- tq_get(fred_tickers, get = "economic.data", from = start_date) %>% 
  dplyr::mutate(symbol = case_when(symbol == fred_tickers[1] ~ "Mining_Production_Index",
                                   symbol == fred_tickers[2] ~ "US_GDP",
                                   symbol == fred_tickers[3] ~ "Truck_Tonnage_Index")) %>% 
  dplyr::rename(series = symbol, value = price)

# I'd recommend incorporating Baltic Dry Index (BDI) for shipping by sea. Unfortunately these are metrics which require subscriptions, and as a result aren't included in this report.
```

```{r, preprocessing, warning = F, message = F, error = F, echo = F}
add_ret_metrics <- function(df_in){
  # Accepts a df with columns value, series, date
  # Adds returns and cumulative return metrics for value.
  df_in %>% 
    group_by(series) %>% 
    arrange(series, date) %>% 
    dplyr::mutate(ret_pct = (value-lag(value))/lag(value),
                  ret_log = log(value/lag(value))) %>% 
    tidyr::drop_na() %>% 
    dplyr::mutate(cret_pct = cumprod(ret_pct+1),
                  cret_log = cumsum(ret_log),
                  year_month = yearmonth(date)) %>% 
    return()
}

price_data <- price_data_in %>% 
  dplyr::select(date, series = symbol, value = close, volume)
# rm(price_data_in)

fundamentals_data <- EIA_Data %>% 
  add_ret_metrics()
# rm(EIA_Data)

# 3:2:1 Crack Spread used here. Setting below can be used to adjust the crack spread to other ratios, such as 5:3:2
spread_ratio <- c(3,2,1)

spread_data <- price_data %>% 
  pivot_wider(names_from = series, id_cols = date, values_from = value) %>% 
  dplyr::mutate(Crack_Spread = ((spread_ratio[2]*42*RBOB + 
                                spread_ratio[3]*42*Heating_Oil) - 
                                WTI*spread_ratio[1])/
                                spread_ratio[1])

price_data <- spread_data %>% dplyr::select(date, value = Crack_Spread) %>% 
  dplyr::mutate(series = "Crack_Spread", volume = 0) %>% 
  bind_rows(price_data) %>% 
  add_ret_metrics()


# EIA_Data %>% pivot_wider(names_from = series, id_cols = date, values_from = value) %>% 
#   dplyr::arrange(desc(date))

fred_data <- fred_data %>% add_ret_metrics()
```

```{r, eda, eval = FALSE, echo = F, warning = F}
# Rolling Correlation of RBOB and WTI
spread_data %>% 
  dplyr::mutate(static_cor = cor(WTI,RBOB),
                cor20 = slide2_dbl(RBOB, WTI, cor, .before = 20, .complete = TRUE),
                cor50 = slide2_dbl(RBOB, WTI, cor, .before = 50, .complete = TRUE),
                cor200 = slide2_dbl(RBOB, WTI, cor, .before = 200, .complete = TRUE)
                ) %>% 
  tidyr::drop_na() %>% 
  ggplot() +
  geom_line(aes(x=date, y = cor20, color = "20-Day Corr")) +
  geom_line(aes(x=date, y = cor50, color = "50-Day Corr")) +
  geom_line(aes(x=date, y = cor200, color = "200-Day Corr")) +
  geom_line(aes(x = date, y = static_cor, color = "Static Corr, Full Horizon")) +
  labs(title = "Rolling Correlations, WTI and RBOB",
       subtitle = "How coupled are spreads to the underlying price?",
       x = "Date",
       y = "Correlation",
       color = "Legend") +
  scale_color_manual(values = c("20-Day Corr" = "red", "50-Day Corr" = "blue", "200-Day Corr" = "green", "Static Corr, Full Horizon" = "black"))

# Natural Gas and Spread Price
spread_data %>% 
  ggplot() +
  geom_line(aes(x = date, y = Crack_Spread), color = "red") +
  geom_line(aes(x = date, y = Natural_Gas), color = "black")

# Crude Stocks
fundamentals_data %>% 
  dplyr::filter(series == "US_Crude_Inventory" & date > start_date) %>% 
  ggplot() +
  geom_line(aes(x = date, y = value)) +
  labs(title = "US Crude Stocks", y = "Thousands of Barrels", x = "Date")

# Crude Stocks, PCT Change
fundamentals_data %>% 
  dplyr::filter(series == "US_Crude_Inventory" & date > start_date) %>% 
  ggplot() +
  geom_line(aes(x = date, y = ret_pct)) +
  labs(title = "US Crude Stocks, Changes in Supply", y = "% Change in Supply", x = "Date")

# Plant Utilization versus Crude Inventories
fundamentals_data %>% 
  pivot_wider(names_from = series, id_cols = date, values_from = value) %>% 
  dplyr::select(Pct_Operable_Utilization, US_Crude_Inventory) %>% 
  tidyr::drop_na() %>% 
  cor()

# This graph reveals short term supply/demand shocks as well as a general inflationary pressure which leads to a gradual increase over the course of the time period.
price_data %>% 
  dplyr::filter(series == "RBOB") %>% 
  ggplot() +
  geom_line(aes(x = date, y = value))
```

```{r, Risk1, echo = F, warning = F}
# price_data %>% 
#   dplyr::filter(series == "Crack_Spread" | series == "RBOB" | series == "WTI" | series == "Heating_Oil") %>% 
#   dplyr::mutate(series = case_when(series == "Crack_Spread" ~ "Crack Spread",
#                                    series == "RBOB" ~ "RBOB",
#                                    series == "WTI" ~ "WTI",
#                                    TRUE ~ "Heating Oil")) %>% 
#   dplyr::filter(ret_pct <= 1, ret_pct >= -1) %>% 
#   ggplot() +
#   geom_histogram(aes(x = ret_pct, y = ..count../sum(..count..)), bins = 100) +
#   labs(x = "Percent Return, Daily", y = "Density", title = "Crack Spread Volatility Compared to its Market Dependencies") +
#   facet_grid(series~.)

vol_comparison_1 <- price_data %>% 
  dplyr::filter(series == "Crack_Spread" | series == "RBOB") %>% 
  dplyr::mutate(series = case_when(series == "Crack_Spread" ~ "Crack Spread",
                                   series == "RBOB" ~ "RBOB",
                                   series == "WTI" ~ "WTI",
                                   TRUE ~ "Heating Oil")) %>% 
  dplyr::filter(ret_pct <= 1, ret_pct >= -1) %>% 
  ggplot() +
  geom_histogram(aes(x = ret_pct, y = ..count../sum(..count..)), bins = 100) +
  labs(x = "Percent Return, Daily", y = "Density", title = "Crack Spread Versus Gasoline Volatility") +
  facet_grid(series~.)

# price_data %>% pivot_wider(id_cols = date, names_from = series, values_from = value) %>% 

vol_comparison_2 <- price_data %>% 
  group_by(series) %>% 
  dplyr::summarise(
    `Average Return` = mean(ret_pct),
    `Standard Deviation` = sd(ret_pct),
    .groups = "keep")

# vol_comparison_1
# vol_comparison_2
```

```{r, FHWA, echo = F, warning = F}
# Pulling distance traveled estimates from FHWA
# https://www.fhwa.dot.gov/policyinformation/travel_monitoring/tvt.cfm
# Potential to create a fully automated workflow. For now, manually link the reports for the time period of interest.

# Must be newest to oldest
links <- c(FHWA2024 = "https://www.fhwa.dot.gov/policyinformation/travel_monitoring/24juntvt/24juntvt.xlsx",
           FHWA2023 = "https://www.fhwa.dot.gov/policyinformation/travel_monitoring/23dectvt/23dectvt.xlsx",
           FHWA2022 = "https://www.fhwa.dot.gov/policyinformation/travel_monitoring/22dectvt/22dectvt.xlsx",
           FHWA2021 = "https://www.fhwa.dot.gov/policyinformation/travel_monitoring/21dectvt/21dectvt.xlsx",
           FHWA2020 = "https://www.fhwa.dot.gov/policyinformation/travel_monitoring/20dectvt/20dectvt.xls",
           FHWA2019 = "https://www.fhwa.dot.gov/policyinformation/travel_monitoring/19dectvt/19dectvt.xls")

month_tags <- c("JAN", "FEB", "MAR", "APR", "MAY", "JUN", "JUL", "AUG", "SEP", "OCT", "NOV", "DEC")

clean_FHWA <- function(df_in){
  
  df_in_year <- paste(df_in %>% head(1), collapse = " ") %>% str_extract(pattern = "[0-9][0-9][0-9][0-9]")
  
  df_in %>% 
    tail(1) %>% 
    pivot_longer(names_to = "month", cols = month_tags) %>% 
    dplyr::select(month, value) %>% 
    dplyr::mutate(year = df_in_year) %>% 
    drop_na() %>% 
    return()
}

for(i in 1:length(links)){
  path <- paste0(names(links)[i],".xlsx")
  curl_download(links[i], path)

  # I am hesitant to use specific row indexes here, due to issues which can arise if the template changes. In   a full implementation I'd add checks for keywords in certain rows.cells to ensure that necessary updates and   mismatches aren't missed.
  
  df1 <- xlsx::read.xlsx(path, sheetName = "Page 3", rowIndex = c(3,4,12))
  df2 <- xlsx::read.xlsx(path, sheetName = "Page 3", rowIndex = c(3,13,20))

  if(i == 1){
    FHWA_Data <- df2 %>% clean_FHWA()
  }
  FHWA_Data <- bind_rows(df1 %>% clean_FHWA(), FHWA_Data)
}

FHWA_Data <- FHWA_Data %>% 
  dplyr::mutate(date = yearmonth(paste(month, year)),
                series = "B_Miles_Driven") %>% 
  dplyr::select(series, date, value) %>% 
  dplyr::mutate(date = as.Date(date),
                value = as.numeric(value)) %>% 
  add_ret_metrics()
  
# This uncovers for us some of the seasonal demand for fuel.
seasonal_demand_1 <- FHWA_Data %>% 
  arrange(date) %>% 
  ggplot() +
  geom_line(aes(x = date, y = value)) +
  labs(title = "Seasonal patterns in road travel, using FHWA Data",
       x = "Date", y = "Billions of Miles Traveled")

# seasonal_demand_1
```

```{r, more eda, eval = F, echo = F, warning = F}
# Re-visualizing data every time I wanted to add another data source was getting inefficient;
# Created a generalized line graph function
s_graph <- function(data, series_name, start_date_in = start_date){
  data %>% 
    dplyr::filter(series == series_name) %>% 
    dplyr::filter(date >= start_date_in) %>% 
    ggplot() +
    geom_line(aes(y = value, x = date)) + 
    labs(title = series_name) %>% 
    return()
}

# Fred
for(s in fred_data$series %>% unique()){
 s_graph(fred_data, s) %>% print()
}

# Price Data
for(s in price_data$series %>% unique()){
 s_graph(price_data, s) %>% print()
}

# Fundamentals Data, EIA
for(s in fundamentals_data$series %>% unique()){
 s_graph(fundamentals_data, s) %>% print()
}

for(s in FHWA_Data$series %>% unique()){
 s_graph(FHWA_Data, s) %>% print()
}
# Notes: Some indicators here track with GDP (Note the strong uptrend. To incorporate these, we may consider normalizing each index to start at 1, and then net the difference between GDP and the other indicator, making a spread)
```

```{r, aggregation, model_prep, echo = F, warning = F}
# Note data misalignment. Reports may publish on non-trading days and therefore we need to approximate lining up time periods. For now, we will use the weekly close, and then match the week a report releases on.

year_week_conversion <- function(df_in){
  df_in %>% 
    dplyr::mutate(year_week = paste0(year(date),"-",week(date) %>% sprintf("%02d", .))) %>% 
    arrange(series, date) %>% 
    group_by(series, year_week) %>% 
    dplyr::mutate(temp = 1, temp = cumsum(temp)) %>% 
    dplyr::filter(temp == max(temp)) %>% 
    return()
}

aggregate <- price_data %>% year_week_conversion()

aggregate <- fundamentals_data %>% year_week_conversion() %>% bind_rows(aggregate)

aggregate <- fred_data %>% year_week_conversion() %>% bind_rows(aggregate)

aggregate <- FHWA_Data %>% year_week_conversion() %>% bind_rows(aggregate)  

wide_agg <- aggregate %>% 
  dplyr::filter(date >= start_date) %>% 
  pivot_wider(id_cols = year_week, names_from = "series", values_from = value) %>% 
  arrange(year_week)

# Creates a re-key using the last date matching a year-week
date_key <- price_data %>% 
  year_week_conversion() %>% 
  ungroup() %>% 
  dplyr::select(date, year_week) %>% 
  arrange(date) %>% 
  group_by(year_week) %>% 
  dplyr::mutate(temp = 1, temp = cumsum(temp)) %>% 
  dplyr::filter(temp == max(temp)) %>% 
  dplyr::select(-temp)

wide_agg <- wide_agg %>% left_join(date_key, by = "year_week")

# Interpolation, Leaving non-updated values the same as previous.
# Opted for linear interpolation due to a lack of high frequency data. Later, we will conduct additional decomposition and ARIMA models for those which exhibit more structured behavior.

# Interpolated
# wide_agg %>% 
#   ungroup() %>% 
#   dplyr::mutate(B_Miles_Driven = na.interp(B_Miles_Driven),
#                 Mining_Production_Index = na.interp(Mining_Production_Index),
#                 Truck_Tonnage_Index = na.interp(Truck_Tonnage_Index),
#                 US_GDP = na.interp(US_GDP),
#                 US_Conventional_Stocks_Unblended = na.interp(US_Conventional_Stocks_Unblended),
#                 US_Crude_Prod = na.interp(US_Crude_Prod),
#                 US_Gasoline_Stocks_Blended = na.interp(US_Gasoline_Stocks_Blended),
#                 US_RBOB_Stocks_Unblended = na.interp(US_RBOB_Stocks_Unblended)
#                 ) %>% 
#   dplyr::select(-date, -year_week, -Heating_Oil, - WTI, -RBOB, -Lower48NGStorage, - Natural_Gas) %>% 
#   drop_na() %>% 
#   cor() %>% 
#   # Credit to ChatGPT for improved formatting on this CorrPlot
#   corrplot( 
#          method = "color",
#          col = colorRampPalette(c("#6D9EC1", "white", "#E46726"))(200),
#          type = "upper",
#          order = "hclust",
#          addCoef.col = "black",
#          tl.col = "black",
#          tl.srt = 45,
#          diag = FALSE,
#          number.cex = 0.6,
#          addshade = "positive",
#          cl.lim = c(-1, 1),
#          mar = c(0, 0, 1, 0)
# )

# Non Interpolated
uninterpolated_data_w_date <- wide_agg %>% 
  ungroup() %>% 
  dplyr::select(-year_week, -Heating_Oil, - WTI, -RBOB, -Lower48NGStorage, - Natural_Gas, - US_GDP) %>%
  drop_na() %>% 
  dplyr::mutate(t = 1, t = cumsum(t))

uninterpolated_data <- uninterpolated_data_w_date %>% dplyr::select(-date, -t)

# uninterpolated_data %>% 
#   cor() %>% 
#   corrplot( 
#          method = "color",
#          col = colorRampPalette(c("#6D9EC1", "white", "#E46726"))(200),
#          type = "upper",
#          order = "hclust",
#          addCoef.col = "black",
#          tl.col = "black",
#          tl.srt = 45,
#          diag = FALSE,
#          number.cex = 0.6,
#          addshade = "positive",
#          cl.lim = c(-1, 1),
#          mar = c(0, 0, 1, 0)
# )
# 
# uninterpolated_data
```

```{r, demonstrational_graphics, echo = F, warning = F}
driver_graph_1 <- fred_data %>% dplyr::filter(series == "Truck_Tonnage_Index") %>% 
  dplyr::mutate(ret_pct = ret_pct * 10,
                cret_pct = cumprod(1+ret_pct),
                cret_pct = (cret_pct+1)*5-5) %>% 
  bind_rows(price_data %>% dplyr::filter(series == "Crack_Spread" | series == "Heating_Oil")) %>% 
  dplyr::filter(date > "2021-01-01") %>% 
  ggplot() +
  geom_line(aes(x = date, y = cret_pct, color = series)) +
  facet_grid(series~.) +
  labs(y = "Index, Range Adjusted",
       title = "Truck Tonnage Influence on Crack Spreads & Heating Oil",
       x = "Date")

# wide_agg %>% 
#   ungroup() %>% 
#   drop_na() %>% 
#   dplyr::select(Truck_Tonnage_Index, Crack_Spread, Heating_Oil) %>% 
#   cor()

# driver_graph_1
```

```{r, linear model, echo = F, warning = F}
# lm1 <- uninterpolated_data %>% lm(Crack_Spread ~ ., data = .)
# lm1 %>% broom::tidy() %>% 
#   arrange(p.value)
# lm1 %>% broom::glance()
# # Not a bad model for pricing predictions, need to address VIF most likely
# 
# lm1 %>% vif() %>% as.data.frame() %>% 
#   dplyr::rename(VIF = 1) %>% 
#   arrange(desc(VIF))
# 
# corr_slice <- function(df_in, series_select){
# df_in %>% 
#   cor() %>% 
#   as.data.frame() %>% 
#   rownames_to_column(var = "series") %>% 
#   dplyr::filter(series == series_select) %>% 
#   t() %>% 
#   as.data.frame() %>% 
#   rename(Correlation = 1) %>% 
#   tail(-1) %>% 
#   arrange(desc(Correlation)) %>% 
#   tail(-1) %>% 
#   return()
# }
# 
# uninterpolated_data %>% corr_slice("Mining_Production_Index")
# uninterpolated_data %>% corr_slice("US_Crude_Prod")

# Dropping Mining Production Index due to high VIF. Because it intersects highly with US_Crude_Prod, our next high VIF, we remodel and reassess.

# uninterpolated_data %>% 
#   dplyr::select(-Mining_Production_Index) %>% 
#   lm(Crack_Spread ~ ., data = .) %>% 
#   vif() %>% 
#   as.data.frame() %>% 
#   dplyr::rename(VIF = 1) %>% 
#   arrange(desc(VIF))

# Crude Inventory is High, however it is a core market driver in our model, and has higher data frequency. As such, we will assess other factors and remove the ones with highest overlap.

# uninterpolated_data %>% 
#   dplyr::select(-Mining_Production_Index) %>% 
#   corr_slice("US_Crude_Inventory")

# We notice here that the highest correlations are to gasoline stocks and petrol imports/exports. We should be okay to drop both of these, since blended gasoline stock is a more compressed portion of the market (smaller number compared to RBOB and Conventional blending formula) and we notice lower correlations in those categories.

# Net imports makes sense. As stocks rise, some companies may import gasoline/diesel to meet rushed demand if they are slightly behind in their refined product inventories, and thus we're better off simply keeping the Crude Inventory Level to reduce the VIF.

# Time to test VIF once more:
data_vif_adj <- uninterpolated_data %>% 
  dplyr::select(-Mining_Production_Index,
                -US_Gasoline_Stocks_Blended,
                -US_Petrol_Products_NI,
                -Pct_Operable_Utilization,
                -US_RBOB_Stocks_Unblended)

lm_vif_adjusted <- data_vif_adj %>% 
  lm(Crack_Spread ~ ., data = .)

final_reg_vif <- lm_vif_adjusted %>% 
  vif() %>% 
  as.data.frame() %>% 
  dplyr::rename(VIF = 1) %>% 
  arrange(desc(VIF))
  
final_reg_p <- lm_vif_adjusted %>% broom::glance()

final_reg_c <- lm_vif_adjusted %>% broom::tidy() %>% 
  arrange(p.value)

# This model appears to be pretty decent for regressions assumptions. Normal Q-Q looks nice, some slight curvature in Residuals vs Fitted, but otherwise this model seems pretty valid.

final_reg_qq <- lm_vif_adjusted %>% autoplot()
data_vif_adj$prediction <- lm_vif_adjusted %>% predict()

data_with_prediction <- data_vif_adj %>% 
  dplyr::mutate(t = 1, t = cumsum(t)) %>% 
  dplyr::left_join(uninterpolated_data_w_date %>% dplyr::select(t,date), by = "t")
  
final_reg_graph <- data_with_prediction %>% 
  dplyr::select(date, prediction, Crack_Spread) %>% 
  dplyr::rename(Prediction = prediction) %>% 
  pivot_longer(names_to = "Legend", cols = c("Prediction", "Crack_Spread")) %>% 
  ggplot(aes(color = Legend)) +
  geom_line(aes(x=date, y=value)) +
  geom_line(aes(x=date, y=value)) +
  labs(title = "Predicting Crack Spread with Fundamental Drivers",
       x = "Date",
       y = "Price",
       legend = "Legend")



# Now for the final step, we need to extrapolate our drivers to today, to predict the crack spread price and compare to the current market price. We can do this in a variety of ways. 

# 1. For random walks, we will hold our assumption constant from the last observed market price.
# 2. For data with more structure, we can do an additive or STL decomposition to remove seasonality and trend, and do an ARMA model on the residuals.

# Note: this is the step where having higher frequency data, through subscriptions and data provider services really pays off. Reducing the extrapolation period can significantly improve model accuracy resulting in a stronger competitive edge.


# Outputs for Appendix:

# final_reg_p
# final_reg_c
# final_reg_qq
# final_reg_graph
# final_reg_vif
```

```{r, echo = F, warning = F}
latest_data_df <- data.frame(temp = NA)
forecast_latest_df <- data.frame(temp = NA)
```

```{r, generalized forecasting function, echo = F, warning = F}
# Let's standardize our forecasting process for the other 8 variables

# Step 1: Filter Data for series, cleanup
# Step 2: Prep TS
# Step 3: Initial ACF/PACF
# Step 4: Fit Model
# Step 5: Summarize and Q-Stat
# Step 6: Residual ACF/PACF
# Step 7: Interpolate Results to Match Current Date    

auto_forecast <- function(data_in, series_name, period_days, show_plot = F){
# Plots and Model Fitting:

output_list <- list()

data_in <- data_in %>% 
  ungroup() %>% 
  dplyr::filter(series == series_name) %>% 
  dplyr::filter(date >= start_date) %>% 
  arrange(date)

if(show_plot){
  line_plot <- data_in %>% 
    ggplot() +
    geom_line(aes(x = date, y = value)) +
    labs(title = series_name)
  
  print(line_plot)
}

ts_in <- data_in %>% 
  dplyr::select(value) %>% 
  ts()

acf1 <- ts_in %>% acf(main = paste("ACF of", series_name), plot = show_plot)
pacf1 <- ts_in %>% pacf(main = paste("PACF of", series_name), plot = show_plot)

fit <- auto.arima(ts_in)
summary(fit)

acf2 <- fit$residuals %>% acf(main = paste("ACF of", series_name, "Residuals"), plot = show_plot)
pacf2 <- fit$residuals %>% pacf(main = paste("PACF of", series_name, "Residuals"), plot = show_plot)

box <- Box.test(fit$residuals, type = "Ljung-Box")

# Horizon, Forecasting and Interpolation

last_date <- data_in %>%
  filter(date == max(date)) %>% 
  .$date

time_diff <- as.numeric(Sys.Date() - last_date)

last_point <- data_in %>%
  filter(date == max(date)) %>% 
  .$value

forecast_length <- ceiling(time_diff/period_days)

#Debug Section, Contains Fixed Assignments. Comment out if not in use

#forecast_length <- 2

# Debug End

if(forecast_length > 0){
  forecast_arima <- forecast(fit, h = forecast_length) %>% as.data.frame()
  
  if(forecast_length == 1){
    #print(1)
    ratio <- time_diff/period_days
    forecast_out <- ratio*forecast_arima$`Point Forecast` + (1-ratio)*last_point
  } else {
    #print(2)
    ratio <- (time_diff/period_days) - (floor(time_diff/period_days))
    forecast_out <- ratio*forecast_arima$`Point Forecast`[forecast_length] + 
      (1-ratio)*forecast_arima$`Point Forecast`[forecast_length-1]
  }
  
} else {
  #print(0)
  forecast_out <- last_point
}

output_list$pf <- forecast_out
output_list$last <- last_point
output_list$data_acf <- acf1
output_list$data_pacf <- pacf1
output_list$res_acf <- acf2
output_list$res_pacf <- pacf2
output_list$q_stat <- box$p.value
output_list$order <- arimaorder(fit)
output_list$fit <- fit

return(output_list)
}

review_f <- function(list_in){
  print(list_in$order)
  print(list_in$q_stat)
}

examine_frequency <- function(df_in, series_name){
  df_in %>% 
    ungroup() %>% 
    arrange(date) %>% 
    filter(series == series_name) %>% 
    return()
}
```

```{r, autoSTL function, echo = F, warning = F}

autoSTL <- function(data_in, series_name, period_days, freq, window_size, show_plot = F, date_filter = start_date){
  
  output_list <- list()

  # Data Prep
  
  ts_in <- data_in %>% 
    dplyr::filter(date >= date_filter) %>% 
    dplyr::filter(series == series_name) %>% 
    ungroup() %>% 
    arrange(date) %>% 
    dplyr::select(value) %>% 
    .$value %>% 
    as.vector() %>% 
    ts(frequency = freq)
  
  data_in <- data_in %>% 
    ungroup() %>% 
    dplyr::filter(series == series_name) %>% 
    dplyr::filter(date >= start_date) %>% 
    arrange(date)
  
  if(show_plot){
  line_plot <- data_in %>% 
    ggplot() +
    geom_line(aes(x = date, y = value)) +
    labs(title = series_name)
  
  print(line_plot)
  }
  
  # Fitting
  
  fit <- stl(ts_in, s.window = window_size)
  
  if(show_plot){
    plot(fit)
  }
  
  # Horizon and Interpolation Prep
  
  last_date <- data_in %>%
  filter(date == max(date)) %>% 
  .$date

  time_diff <- as.numeric(Sys.Date() - last_date)

  last_point <- data_in %>%
    filter(date == max(date)) %>% 
    .$value

  forecast_length <- ceiling(time_diff/period_days)
  
  #Debug Section, Contains Fixed Assignments. Comment out if not in use

  # forecast_length <- 0

  # Debug End
  
  # Forecast and Interpolation

  if(forecast_length > 0){
  forecast_stl <- forecast(fit, h = forecast_length) %>% as.data.frame()
  
    if(forecast_length == 1){
    #print(1)
      ratio <- time_diff/period_days
      forecast_out <- ratio*forecast_stl$`Point Forecast` + (1-ratio)*last_point
    } else {
    #print(2)
      ratio <- (time_diff/period_days) - (floor(time_diff/period_days))
      forecast_out <- ratio*forecast_stl$`Point Forecast`[forecast_length] + 
        (1-ratio)*forecast_stl$`Point Forecast`[forecast_length-1]
    }
  
  } else {
  #print(0)
    forecast_out <- last_point
  }
  
  output_list$pf <- forecast_out
  output_list$last <- last_point
  output_list$fit <- fit
  
  res_df <- fit$time.series[, "remainder"] %>% as.data.frame()
  
  output_list$arima_ready <- cbind(data_in, res_df) %>% dplyr::mutate(value = x)
  
  return(output_list)
}

```

```{r, Forecasting, echo = F, warning = F}
show_all_charts <- FALSE

#Appears to be a randnom walk, using direct ARIMA approach to integrate the model.
crude_inv_model <- auto_forecast(fundamentals_data, "US_Crude_Inventory", 7, show_all_charts)
latest_data_df$US_Crude_Inventory <- crude_inv_model$last
forecast_latest_df$US_Crude_Inventory <- crude_inv_model$pf

tonnage_model <- auto_forecast(fred_data, "Truck_Tonnage_Index", 365/12, show_all_charts)
#tonnage_model %>% review_f()
latest_data_df$Truck_Tonnage_Index <- tonnage_model$last
forecast_latest_df$Truck_Tonnage_Index <- tonnage_model$pf
  
crude_prod_stl <- fundamentals_data %>% autoSTL(series_name = "US_Crude_Prod", 365/12, 12, 13, show_all_charts)
crude_prod_arma <- crude_prod_stl$arima_ready %>% auto_forecast("US_Crude_Prod", 365/12, show_all_charts)
#crude_prod_arma %>% review_f()
latest_data_df$US_Crude_Prod <- crude_prod_stl$last
forecast_latest_df$US_Crude_Prod <- crude_prod_stl$pf + crude_prod_arma$pf

# FHWA_Data %>% examine_frequency("B_Miles_Driven")
miles_stl <- FHWA_Data %>% autoSTL(series_name = "B_Miles_Driven", 365/12, 12, 13, show_all_charts)
miles_arima <- miles_stl$arima_ready %>% auto_forecast("B_Miles_Driven", 365/12, show_all_charts)
#miles_arima %>% review_f()
latest_data_df$B_Miles_Driven <- miles_stl$last
forecast_latest_df$B_Miles_Driven <- miles_stl$pf + miles_arima$pf

# US_Gasoline_Production
# fundamentals_data %>% examine_frequency("US_Gasoline_Production")
gas_prod_stl <- fundamentals_data %>% autoSTL("US_Gasoline_Production", 7, 52, 53, show_all_charts)
gas_prod_arima <- gas_prod_stl$arima_ready %>% auto_forecast("US_Gasoline_Production", 7, show_all_charts)
#gas_prod_arima %>% review_f()
latest_data_df$US_Gasoline_Production <- gas_prod_stl$last
forecast_latest_df$US_Gasoline_Production <- gas_prod_stl$pf + gas_prod_arima$pf

# fundamentals_data %>% examine_frequency("US_Crude_NI")
crude_ni_stl <- fundamentals_data %>% autoSTL("US_Crude_NI", 7, 52, 53, show_all_charts)
crude_ni_arima <- crude_ni_stl$arima_ready %>% auto_forecast("US_Crude_NI", 7, show_all_charts)
#crude_ni_arima %>% review_f()
latest_data_df$US_Crude_NI <- crude_ni_stl$last
# model fit with 0 order, omitted from results
forecast_latest_df$US_Crude_NI <- crude_ni_stl$pf

# fundamentals_data %>% examine_frequency("US_Conventional_Stocks_Unblended")
con_stock_stl <- fundamentals_data %>% autoSTL("US_Conventional_Stocks_Unblended", 365/12, 12, 13, show_all_charts)
con_stock_arima <- con_stock_stl$arima_ready %>% auto_forecast("US_Conventional_Stocks_Unblended", 365/12, show_all_charts)
#con_stock_arima %>% review_f()
latest_data_df$US_Conventional_Stocks_Unblended <- con_stock_stl$last
forecast_latest_df$US_Conventional_Stocks_Unblended <-  con_stock_stl$pf + con_stock_arima$pf

#VIF too high, and p-value's too low. Removed

# fundamentals_data %>% examine_frequency("Pct_Operable_Utilization")
# util_stl <- fundamentals_data %>% autoSTL("Pct_Operable_Utilization", 7, 52, 53, show_all_charts)
# util_arima <- util_stl$arima_ready %>% auto_forecast("Pct_Operable_Utilization", 7, show_all_charts)
#util_arima %>% review_f()
# latest_data_df$Pct_Operable_Utilization <- util_stl$last
# forecast_latest_df$Pct_Operable_Utilization <- util_stl$pf + util_arima$pf

# fundamentals_data %>% examine_frequency("US_RBOB_Stocks_Unblended")
# rbob_un_stl <- fundamentals_data %>% autoSTL("US_RBOB_Stocks_Unblended", 365/12, 12, 13, show_all_charts)
# rbob_un_arima <- rbob_un_stl$arima_ready %>% auto_forecast("US_RBOB_Stocks_Unblended", 365/12, show_all_charts)
# #rbob_un_arima %>% review_f()
# latest_data_df$US_RBOB_Stocks_Unblended <- rbob_un_stl$last
# # arima omitted, no fit
# forecast_latest_df$US_RBOB_Stocks_Unblended <- rbob_un_stl$pf
  
```

```{r, model recommendation, echo = F, warning = F}
forecast_prediction <- predict(lm_vif_adjusted, forecast_latest_df %>% dplyr::select(-temp)) %>% round(digits = 2)

prediction_last_observed <- predict(lm_vif_adjusted, latest_data_df %>% dplyr::select(-temp)) %>% round(digits = 2)

current_price <- price_data %>%
  filter(series == "Crack_Spread") %>% 
  filter(date == max(date)) %>% 
  .$value %>% 
  round(digits = 2)

if(forecast_prediction > current_price){
  recommendation <- "long"
} else {
  recommendation <- "short"
}

if(forecast_prediction > current_price){
  recommendation2 <- "bullish"
} else {
  recommendation2 <- "bearish"
}
```

## Recommendation

The 3-2-1 crack spread looks `r recommendation2` based on market fundamentals. Our model predicts a crack spread of `r forecast_prediction` using forecasted values and `r prediction_last_observed` using the last observable data points. Based on the most recent 3-2-1 crack spread price of `r current_price`, a `r recommendation` position in the crack spread is recommended.

## Trading Strategy

A long position in the crack spread can be entered in one of two ways: 

1. Enter using a long crack spread futures contract. By locking in the price of the crack spread today, we will profit if the crack spread increases.

2. Enter long into 2 gasoline and 1 heating oil futures contract, and then enter short into 3 crude oil contracts. Note that crude oil contract volumes are 1000 barrels on the CME and RBOB and Heating Oil volumes are 42,000 gallons (1000 Barrels).

Note: In the event market conditions change, and a short position is recommended, enter the opposite of the above position.

## Rationale

```{r, echo = FALSE}
final_reg_graph
```

The prediction is driven by a regression model (shown above) consisting of **9 market drivers**, with sources from the Energy Information Administration, Federal Reserve Economic Data, Yahoo Finance, and Federal Highway administration. Since the **regression model** makes use of aligned monthly data points, without interpolation, it's better used when entering longer time frame positions with a clear stop loss and exit strategy. This model attempts to predict current prices using current fundamental market levels. The key idea is that when prices and our predictions are out of sync, the price will normalize to our current model prediction in the future. This lets us choose which position to enter based on whether our prediction is higher or lower than the current price.

Because our predictors are updated at different frequencies and different time lags, forecasting models are developed for each indicator. **STL decomposition** is used to capture larger structural patterns such as seasonality and trend, and **ARIMA models** are then used to capture remaining dependency in the residuals. This allows us to predict today's fundamentals levels based on previous seasonality and trend. Because time series forecasting expands in uncertainty the further forward in time you forecast, a prediction using the last observable data points is also provided. This lets us compare how much the time series forecast is influencing the recommended decision relative to the last observable data.

## Fundamental Market Drivers

Data was selected by a combination of higher correlations with the crack spread prices as well as higher data frequency supporting more timely decision making. See "How does each driver factor into forecasted price?" in the appendix for a more detailed breakdown.

**Demand Side (Bullish) Factors:** 

* Truck Tonnage 
* Billion Miles Driven
* US Crude Net Imports

**Supply Side (Bearish) Factors:**

* US Crude Inventory 
* US Crude Production 
* US Gasoline Production
* US Crude Net Imports
* Percent Operable Utilization (Refineries) 
* Gasoline Stocks, Conventional

RBOB Stocks and Plant Utilization were also considered, but removed due to high VIF and low P Values.

## Risk/Reward

Model Risk:

Different time periods may have different equilibriums, even if all market drivers are at the same level. For example, technology can make production and refinement cheaper, which in turn means refineries are willing to produce more under less profitable conditions, which would drive the spread closer together. It is important to consult the team to determine what a competitive time frame entails. If the time frame in fact needs to be shortened, longer time frames can be used for the market drivers to capture long term seasonal effects, and shorter time frames can be used to train the underlying regression model.

A second form of model rick comes from how linear regressions function. We have assumed here that our selected variables drive the crack spread, and that crack spread is the dependent variable. We know in fact, that this is not true. The various prices and supply/demand levels are cyclic. A high crack spread may incentivize processing at a faster pace, and thus affecting existing inventories. Existing inventories being high or low can affect the crack spread. In some cases our model will respond to the levels of the independent variables, where we see the crack spread converge to the expected level, and in some cases, our independent variables will change, adjusting our prediction.

With the previous factors in mind, assuming no reversals between dependent and independent variables, our model forecasts an upside potential of `r abs(current_price - forecast_prediction)` per barrel.

How then do we assess the underlying risk, knowing that our model's effectiveness can be disrupted? Focusing on the riskiness of the underlying asset is crucial, since it gives us an unbiased standpoint separate from our model.

```{r, echo = FALSE, warning = F}
#VAR Metrics
var_monthly <- price_data %>% 
  dplyr::filter(series == "Crack_Spread") %>% 
  dplyr::mutate(monthly_cret = slide_dbl(ret_pct+1, prod, .before = 20, .complete = TRUE),
                monthly_cret = monthly_cret - 1) %>% 
  drop_na()

var_monthly_hist <- var_monthly %>% 
  dplyr::filter(abs(monthly_cret) < 2) %>% 
  ggplot() +
  geom_histogram(aes(x = monthly_cret, y = ..count../sum(..count..)), bins = 100) +
  labs(title = "Distribution of Monthly Returns, Crack Spread",
       x = "Monthly Return",
       y = "Density")

threshold <- 0.05

if(recommendation == "long"){
  VAR <- var_monthly %>% 
    dplyr::filter(monthly_cret <= quantile(monthly_cret, threshold)) %>% 
    dplyr::filter(monthly_cret == max(monthly_cret)) %>% 
    .$monthly_cret %>% 
    round(digits = 3)
} else {
  VAR <- var_monthly %>% 
    dplyr::filter(monthly_cret >= quantile(monthly_cret, (1-threshold))) %>% 
    dplyr::filter(monthly_cret == min(monthly_cret)) %>% 
    .$monthly_cret %>% 
    round(digits = 3)
  VAR <- VAR*-1
}
```

The 5% Value at Risk on 1 month positions in the crack spread is `r VAR`. From this we can estimate that our best outcome in the worst 5% of cases is `r VAR` on a 1 month position. Tail risk is more extreme for short positions than long positions, supported by the data. A more detailed view of the monthly return distribution for crack spread is shown below.

```{r, echo = FALSE, warning = F}
# Display
var_monthly_hist
```
To assess liquidity risk, the risk of getting closed out of a position due to higher daily volatility, see the rolling standard deviation chart below. It demonstrates both decreasing volatility over, but also the tendency for volatility to spike for short periods:

```{r, echo = FALSE}
#Rolling Volatility Crack Spread
rolling_vol <- price_data %>% 
  dplyr::filter(series == "Crack_Spread") %>% 
  dplyr::filter(date >= "2022-01-01") %>% 
  dplyr::mutate(sd20 = slide_dbl(ret_pct, sd, .before = 20, .complete = TRUE)) %>% 
  dplyr::mutate(sd50 = slide_dbl(ret_pct, sd, .before = 50, .complete = TRUE)) %>% 
  dplyr::mutate(sd200 = slide_dbl(ret_pct, sd, .before = 200, .complete = TRUE)) %>% 
  dplyr::mutate(static_sd = sd(ret_pct)) %>% 
  dplyr::ungroup() %>% 
  dplyr::select(date, sd20, sd50, sd200, static_sd) %>% 
  pivot_longer(cols = c("sd20", "sd50", "sd200", "static_sd")) %>% 
  ggplot(aes(color = name)) +
  geom_line(aes(x = date, y = value)) +
  labs(y = "Volatility of Returns",
       x = "Date",
       title = "Rolling Volatilities of Crack Spread",
       subtitle = "20 day, 50 Day, 200 Day and Average")

rolling_vol
```

From this we can conclude that our best outcome in the worst 5% of cases is `r VAR` and that our daily volatility is much lower at this point of time, meaning less margin is required to sustain a 1 month position to close. If volatility increases long term, a model which focuses on a narrower time frame would likely be more suitable.

## Appendix

### Regression, Primary Model

```{r, echo = FALSE}
final_reg_graph
final_reg_p %>% dplyr::select(R_Squared = r.squared, 
                              Adj_R_Squared = adj.r.squared, 
                              SD = sigma, 
                              P_Val = p.value, 
                              Degrees_Freedom = df, 
                              AIC, 
                              BIC) %>% kable()
final_reg_c %>% 
  dplyr::select(Market_Driver = term, Coefficient = estimate, Standard_Error = std.error, P_Val = p.value) %>% 
  kable()

final_reg_qq
final_reg_vif %>% kable()
```

### How does each driver factor into forecasted price?

```{r, echo = FALSE}
forecast_latest_df$`(Intercept)` <- 1
df1 <- forecast_latest_df %>% t() %>% as.data.frame() %>% rownames_to_column(var = "Market_Driver") %>% 
  drop_na()
df2 <- final_reg_c %>% 
  dplyr::select(Market_Driver = term, Coefficient = estimate, Standard_Error = std.error, P_Val = p.value)
df3 <- left_join(df2,df1,by="Market_Driver") %>% dplyr::mutate(Value = Coefficient * V1) %>% 
  dplyr::select(Market_Driver, Value)

df3 %>% kable()
df3 %>% summarise(Estimate = sum(Value)) %>% kable()

```

### Risk Assessment Visuals

```{r, echo = F, warning = F, message = F}
vol_comparison_1 + labs(subtitle = "Crack Spread is more Volatile than RBOB")
var_monthly_hist
rolling_vol
```

```{r, Developer Notes, echo = FALSE}
# A place for the developer to note potential features and future directions to take the model.

# Can today's crack spread be used to predict tomorrows crude price? Perhaps is the crack spread is favorable enough we can expect a bullish run on crude as companies seek to increase production to capitalize on higher margins.

# Note the move to v2 API call on EIA

# 3 potential options for modelling future price: 
  # Regression model, and STI forecasts of dependent variables.
  # Regression model making predictions using lagged variables.
  # Regression model that predicts today price, and the difference between the prediction and the actual       # price is used.
  #The third option can be improved with Jarque-Bera distribution on the error terms, to determine how "off" a particular day is.

# Option 3 is preferable, since it can give us a band of expected prices today, and help us get a better sense of a strong/weak reversal potential.
```
