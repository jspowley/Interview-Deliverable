dplyr::mutate(year = df_in_year) %>%
drop_na() %>%
return()
}
for(i in 1:length(links)){
if(str_detect(links[i], pattern = ".xlsx")){
ext <- ".xlsx"
} else {
ext <- ".xls"
}
path <- paste0(names(links)[i], ext)
curl_download(links[i], path)
# I am hesitant to use specific row indexes here, due to issues which can arise if the template changes. In   a full     implementation I'd add checks for keywords in certain rows.cells to ensure that necessary updates and   mismatches aren't missed.
# Java dependency fix called for quick adjustments prior to delivery
if(ext == ".xlsx"){
df1 <- openxlsx::read.xlsx(path, sheet = "Page 3", rows = c(3,4,12))
df2 <- openxlsx::read.xlsx(path, sheet = "Page 3", rows = c(3,13,20))
} else {
df1 <- read_excel(path, sheet = "Page 3")
df1 <- df1[c(2,3,11),] %>%
dplyr::select(c(-1,-2,-3,-16))
colnames(df1) <- df1[1, ]
df1 <- df1 %>% tail(-1)
df2 <- read_excel(path, sheet = "Page 3")
df2 <- df2[c(2,12,19),] %>%
dplyr::select(c(-1,-2,-3,-16))
colnames(df2) <- df2[1, ]
df2 <- df2 %>% tail(-1)
}
if(i == 1){
FHWA_Data <- df2 %>% clean_FHWA()
}
FHWA_Data <- bind_rows(df1 %>% clean_FHWA(), FHWA_Data)
}
FHWA_Data <- FHWA_Data %>%
dplyr::mutate(date = yearmonth(paste(month, year)),
series = "B_Miles_Driven") %>%
dplyr::select(series, date, value) %>%
dplyr::mutate(date = as.Date(date),
value = as.numeric(value)) %>%
add_ret_metrics()
# This uncovers for us some of the seasonal demand for fuel.
seasonal_demand_1 <- FHWA_Data %>%
arrange(date) %>%
ggplot() +
geom_line(aes(x = date, y = value)) +
labs(title = "Seasonal patterns in road travel, using FHWA Data",
x = "Date", y = "Billions of Miles Traveled")
# seasonal_demand_1
FHWA_Data %>% s_graph("B_Miles_Driven")
# Re-visualizing data every time I wanted to add another data source was getting inefficient;
# Created a generalized line graph function
s_graph <- function(data, series_name, start_date_in = start_date){
data %>%
dplyr::filter(series == series_name) %>%
dplyr::filter(date >= start_date_in) %>%
ggplot() +
geom_line(aes(y = value, x = date)) +
labs(title = series_name) %>%
return()
}
# Fred
for(s in fred_data$series %>% unique()){
s_graph(fred_data, s) %>% print()
}
# Price Data
for(s in price_data$series %>% unique()){
s_graph(price_data, s) %>% print()
}
# Fundamentals Data, EIA
for(s in fundamentals_data$series %>% unique()){
s_graph(fundamentals_data, s) %>% print()
}
for(s in FHWA_Data$series %>% unique()){
s_graph(FHWA_Data, s) %>% print()
}
# Notes: Some indicators here track with GDP (Note the strong uptrend. To incorporate these, we may consider normalizing each index to start at 1, and then net the difference between GDP and the other indicator, making a spread)
# Note data misalignment. Reports may publish on non-trading days and therefore we need to approximate lining up time periods. For now, we will use the weekly close, and then match the week a report releases on.
year_week_conversion <- function(df_in){
df_in %>%
dplyr::mutate(year_week = paste0(year(date),"-",week(date) %>% sprintf("%02d", .))) %>%
arrange(series, date) %>%
group_by(series, year_week) %>%
dplyr::mutate(temp = 1, temp = cumsum(temp)) %>%
dplyr::filter(temp == max(temp)) %>%
return()
}
aggregate <- price_data %>% year_week_conversion()
aggregate <- fundamentals_data %>% year_week_conversion() %>% bind_rows(aggregate)
aggregate <- fred_data %>% year_week_conversion() %>% bind_rows(aggregate)
aggregate <- FHWA_Data %>% year_week_conversion() %>% bind_rows(aggregate)
wide_agg <- aggregate %>%
dplyr::filter(date >= start_date) %>%
pivot_wider(id_cols = year_week, names_from = "series", values_from = value) %>%
arrange(year_week)
# Creates a re-key using the last date matching a year-week
date_key <- price_data %>%
year_week_conversion() %>%
ungroup() %>%
dplyr::select(date, year_week) %>%
arrange(date) %>%
group_by(year_week) %>%
dplyr::mutate(temp = 1, temp = cumsum(temp)) %>%
dplyr::filter(temp == max(temp)) %>%
dplyr::select(-temp)
wide_agg <- wide_agg %>% left_join(date_key, by = "year_week")
# Interpolation, Leaving non-updated values the same as previous.
# Opted for linear interpolation due to a lack of high frequency data. Later, we will conduct additional decomposition and ARIMA models for those which exhibit more structured behavior.
# Interpolated
# wide_agg %>%
#   ungroup() %>%
#   dplyr::mutate(B_Miles_Driven = na.interp(B_Miles_Driven),
#                 Mining_Production_Index = na.interp(Mining_Production_Index),
#                 Truck_Tonnage_Index = na.interp(Truck_Tonnage_Index),
#                 US_GDP = na.interp(US_GDP),
#                 US_Conventional_Stocks_Unblended = na.interp(US_Conventional_Stocks_Unblended),
#                 US_Crude_Prod = na.interp(US_Crude_Prod),
#                 US_Gasoline_Stocks_Blended = na.interp(US_Gasoline_Stocks_Blended),
#                 US_RBOB_Stocks_Unblended = na.interp(US_RBOB_Stocks_Unblended)
#                 ) %>%
#   dplyr::select(-date, -year_week, -Heating_Oil, - WTI, -RBOB, -Lower48NGStorage, - Natural_Gas) %>%
#   drop_na() %>%
#   cor() %>%
#   # Credit to ChatGPT for improved formatting on this CorrPlot
#   corrplot(
#          method = "color",
#          col = colorRampPalette(c("#6D9EC1", "white", "#E46726"))(200),
#          type = "upper",
#          order = "hclust",
#          addCoef.col = "black",
#          tl.col = "black",
#          tl.srt = 45,
#          diag = FALSE,
#          number.cex = 0.6,
#          addshade = "positive",
#          cl.lim = c(-1, 1),
#          mar = c(0, 0, 1, 0)
# )
# Non Interpolated
uninterpolated_data_w_date <- wide_agg %>%
ungroup() %>%
dplyr::select(-year_week, -Heating_Oil, - WTI, -RBOB, -Lower48NGStorage, - Natural_Gas, - US_GDP) %>%
drop_na() %>%
dplyr::mutate(t = 1, t = cumsum(t))
uninterpolated_data <- uninterpolated_data_w_date %>% dplyr::select(-date, -t)
# uninterpolated_data %>%
#   cor() %>%
#   corrplot(
#          method = "color",
#          col = colorRampPalette(c("#6D9EC1", "white", "#E46726"))(200),
#          type = "upper",
#          order = "hclust",
#          addCoef.col = "black",
#          tl.col = "black",
#          tl.srt = 45,
#          diag = FALSE,
#          number.cex = 0.6,
#          addshade = "positive",
#          cl.lim = c(-1, 1),
#          mar = c(0, 0, 1, 0)
# )
#
# uninterpolated_data
driver_graph_1 <- fred_data %>% dplyr::filter(series == "Truck_Tonnage_Index") %>%
dplyr::mutate(ret_pct = ret_pct * 10,
cret_pct = cumprod(1+ret_pct),
cret_pct = (cret_pct+1)*5-5) %>%
bind_rows(price_data %>% dplyr::filter(series == "Crack_Spread" | series == "Heating_Oil")) %>%
dplyr::filter(date > "2021-01-01") %>%
ggplot() +
geom_line(aes(x = date, y = cret_pct, color = series)) +
facet_grid(series~.) +
labs(y = "Index, Range Adjusted",
title = "Truck Tonnage Influence on Crack Spreads & Heating Oil",
x = "Date")
# wide_agg %>%
#   ungroup() %>%
#   drop_na() %>%
#   dplyr::select(Truck_Tonnage_Index, Crack_Spread, Heating_Oil) %>%
#   cor()
# driver_graph_1
# lm1 <- uninterpolated_data %>% lm(Crack_Spread ~ ., data = .)
# lm1 %>% broom::tidy() %>%
#   arrange(p.value)
# lm1 %>% broom::glance()
# # Not a bad model for pricing predictions, need to address VIF most likely
#
# lm1 %>% vif() %>% as.data.frame() %>%
#   dplyr::rename(VIF = 1) %>%
#   arrange(desc(VIF))
#
# corr_slice <- function(df_in, series_select){
# df_in %>%
#   cor() %>%
#   as.data.frame() %>%
#   rownames_to_column(var = "series") %>%
#   dplyr::filter(series == series_select) %>%
#   t() %>%
#   as.data.frame() %>%
#   rename(Correlation = 1) %>%
#   tail(-1) %>%
#   arrange(desc(Correlation)) %>%
#   tail(-1) %>%
#   return()
# }
#
# uninterpolated_data %>% corr_slice("Mining_Production_Index")
# uninterpolated_data %>% corr_slice("US_Crude_Prod")
# Dropping Mining Production Index due to high VIF. Because it intersects highly with US_Crude_Prod, our next high VIF, we remodel and reassess.
# uninterpolated_data %>%
#   dplyr::select(-Mining_Production_Index) %>%
#   lm(Crack_Spread ~ ., data = .) %>%
#   vif() %>%
#   as.data.frame() %>%
#   dplyr::rename(VIF = 1) %>%
#   arrange(desc(VIF))
# Crude Inventory is High, however it is a core market driver in our model, and has higher data frequency. As such, we will assess other factors and remove the ones with highest overlap.
# uninterpolated_data %>%
#   dplyr::select(-Mining_Production_Index) %>%
#   corr_slice("US_Crude_Inventory")
# We notice here that the highest correlations are to gasoline stocks and petrol imports/exports. We should be okay to drop both of these, since blended gasoline stock is a more compressed portion of the market (smaller number compared to RBOB and Conventional blending formula) and we notice lower correlations in those categories.
# Net imports makes sense. As stocks rise, some companies may import gasoline/diesel to meet rushed demand if they are slightly behind in their refined product inventories, and thus we're better off simply keeping the Crude Inventory Level to reduce the VIF.
# Time to test VIF once more:
data_vif_adj <- uninterpolated_data %>%
dplyr::select(-Mining_Production_Index,
-US_Gasoline_Stocks_Blended,
-US_Petrol_Products_NI,
-Pct_Operable_Utilization,
-US_RBOB_Stocks_Unblended)
lm_vif_adjusted <- data_vif_adj %>%
lm(Crack_Spread ~ ., data = .)
final_reg_vif <- lm_vif_adjusted %>%
vif() %>%
as.data.frame() %>%
dplyr::rename(VIF = 1) %>%
arrange(desc(VIF))
final_reg_p <- lm_vif_adjusted %>% broom::glance()
final_reg_c <- lm_vif_adjusted %>% broom::tidy() %>%
arrange(p.value)
# This model appears to be pretty decent for regressions assumptions. Normal Q-Q looks nice, some slight curvature in Residuals vs Fitted, but otherwise this model seems pretty valid.
final_reg_qq <- lm_vif_adjusted %>% autoplot()
data_vif_adj$prediction <- lm_vif_adjusted %>% predict()
data_with_prediction <- data_vif_adj %>%
dplyr::mutate(t = 1, t = cumsum(t)) %>%
dplyr::left_join(uninterpolated_data_w_date %>% dplyr::select(t,date), by = "t")
final_reg_graph <- data_with_prediction %>%
dplyr::select(date, prediction, Crack_Spread) %>%
dplyr::rename(Prediction = prediction) %>%
pivot_longer(names_to = "Legend", cols = c("Prediction", "Crack_Spread")) %>%
ggplot(aes(color = Legend)) +
geom_line(aes(x=date, y=value)) +
labs(title = "Predicting Crack Spread with Fundamental Drivers",
x = "Date",
y = "Price",
legend = "Legend")
# Now for the final step, we need to extrapolate our drivers to today, to predict the crack spread price and compare to the current market price. We can do this in a variety of ways.
# 1. For random walks, we will hold our assumption constant from the last observed market price.
# 2. For data with more structure, we can do an additive or STL decomposition to remove seasonality and trend, and do an ARMA model on the residuals.
# Note: this is the step where having higher frequency data, through subscriptions and data provider services really pays off. Reducing the extrapolation period can significantly improve model accuracy resulting in a stronger competitive edge.
# Outputs for Appendix:
# final_reg_p
# final_reg_c
# final_reg_qq
# final_reg_graph
# final_reg_vif
latest_data_df <- data.frame(temp = NA)
forecast_latest_df <- data.frame(temp = NA)
# Let's standardize our forecasting process for the other 8 variables
# Step 1: Filter Data for series, cleanup
# Step 2: Prep TS
# Step 3: Initial ACF/PACF
# Step 4: Fit Model
# Step 5: Summarize and Q-Stat
# Step 6: Residual ACF/PACF
# Step 7: Interpolate Results to Match Current Date
auto_forecast <- function(data_in, series_name, period_days, show_plot = F){
# Plots and Model Fitting:
output_list <- list()
data_in <- data_in %>%
ungroup() %>%
dplyr::filter(series == series_name) %>%
dplyr::filter(date >= start_date) %>%
arrange(date)
if(show_plot){
line_plot <- data_in %>%
ggplot() +
geom_line(aes(x = date, y = value)) +
labs(title = series_name)
print(line_plot)
}
ts_in <- data_in %>%
dplyr::select(value) %>%
ts()
acf1 <- ts_in %>% acf(main = paste("ACF of", series_name), plot = show_plot)
pacf1 <- ts_in %>% pacf(main = paste("PACF of", series_name), plot = show_plot)
fit <- auto.arima(ts_in)
summary(fit)
acf2 <- fit$residuals %>% acf(main = paste("ACF of", series_name, "Residuals"), plot = show_plot)
pacf2 <- fit$residuals %>% pacf(main = paste("PACF of", series_name, "Residuals"), plot = show_plot)
box <- Box.test(fit$residuals, type = "Ljung-Box")
# Horizon, Forecasting and Interpolation
last_date <- data_in %>%
filter(date == max(date)) %>%
.$date
time_diff <- as.numeric(Sys.Date() - last_date)
last_point <- data_in %>%
filter(date == max(date)) %>%
.$value
forecast_length <- ceiling(time_diff/period_days)
#Debug Section, Contains Fixed Assignments. Comment out if not in use
#forecast_length <- 2
# Debug End
if(forecast_length > 0){
forecast_arima <- forecast(fit, h = forecast_length) %>% as.data.frame()
if(forecast_length == 1){
#print(1)
ratio <- time_diff/period_days
forecast_out <- ratio*forecast_arima$`Point Forecast` + (1-ratio)*last_point
} else {
#print(2)
ratio <- (time_diff/period_days) - (floor(time_diff/period_days))
forecast_out <- ratio*forecast_arima$`Point Forecast`[forecast_length] +
(1-ratio)*forecast_arima$`Point Forecast`[forecast_length-1]
}
} else {
#print(0)
forecast_out <- last_point
}
output_list$pf <- forecast_out
output_list$last <- last_point
output_list$data_acf <- acf1
output_list$data_pacf <- pacf1
output_list$res_acf <- acf2
output_list$res_pacf <- pacf2
output_list$q_stat <- box$p.value
output_list$order <- arimaorder(fit)
output_list$fit <- fit
return(output_list)
}
review_f <- function(list_in){
print(list_in$order)
print(list_in$q_stat)
}
examine_frequency <- function(df_in, series_name){
df_in %>%
ungroup() %>%
arrange(date) %>%
filter(series == series_name) %>%
return()
}
autoSTL <- function(data_in, series_name, period_days, freq, window_size, show_plot = F, date_filter = start_date){
output_list <- list()
# Data Prep
ts_in <- data_in %>%
dplyr::filter(date >= date_filter) %>%
dplyr::filter(series == series_name) %>%
ungroup() %>%
arrange(date) %>%
dplyr::select(value) %>%
.$value %>%
as.vector() %>%
ts(frequency = freq)
data_in <- data_in %>%
ungroup() %>%
dplyr::filter(series == series_name) %>%
dplyr::filter(date >= start_date) %>%
arrange(date)
if(show_plot){
line_plot <- data_in %>%
ggplot() +
geom_line(aes(x = date, y = value)) +
labs(title = series_name, x = "Date", y = "Value")
print(line_plot)
}
# Fitting
fit <- stl(ts_in, s.window = window_size)
if(show_plot){
plot(fit)
}
# Horizon and Interpolation Prep
last_date <- data_in %>%
filter(date == max(date)) %>%
.$date
time_diff <- as.numeric(Sys.Date() - last_date)
last_point <- data_in %>%
filter(date == max(date)) %>%
.$value
forecast_length <- ceiling(time_diff/period_days)
#Debug Section, Contains Fixed Assignments. Comment out if not in use
# forecast_length <- 0
# Debug End
# Forecast and Interpolation
if(forecast_length > 0){
forecast_stl <- forecast(fit, h = forecast_length) %>% as.data.frame()
if(forecast_length == 1){
#print(1)
ratio <- time_diff/period_days
forecast_out <- ratio*forecast_stl$`Point Forecast` + (1-ratio)*last_point
} else {
#print(2)
ratio <- (time_diff/period_days) - (floor(time_diff/period_days))
forecast_out <- ratio*forecast_stl$`Point Forecast`[forecast_length] +
(1-ratio)*forecast_stl$`Point Forecast`[forecast_length-1]
}
} else {
#print(0)
forecast_out <- last_point
}
output_list$pf <- forecast_out
output_list$last <- last_point
output_list$fit <- fit
res_df <- fit$time.series[, "remainder"] %>% as.data.frame()
output_list$arima_ready <- cbind(data_in, res_df) %>% dplyr::mutate(value = x)
return(output_list)
}
show_all_charts <- FALSE
#Appears to be a randnom walk, using direct ARIMA approach to integrate the model.
crude_inv_model <- auto_forecast(fundamentals_data, "US_Crude_Inventory", 7, show_all_charts)
latest_data_df$US_Crude_Inventory <- crude_inv_model$last
forecast_latest_df$US_Crude_Inventory <- crude_inv_model$pf
tonnage_model <- auto_forecast(fred_data, "Truck_Tonnage_Index", 365/12, show_all_charts)
#tonnage_model %>% review_f()
latest_data_df$Truck_Tonnage_Index <- tonnage_model$last
forecast_latest_df$Truck_Tonnage_Index <- tonnage_model$pf
crude_prod_stl <- fundamentals_data %>% autoSTL(series_name = "US_Crude_Prod", 365/12, 12, 13, show_all_charts)
crude_prod_arma <- crude_prod_stl$arima_ready %>% auto_forecast("US_Crude_Prod", 365/12, show_all_charts)
#crude_prod_arma %>% review_f()
latest_data_df$US_Crude_Prod <- crude_prod_stl$last
forecast_latest_df$US_Crude_Prod <- crude_prod_stl$pf + crude_prod_arma$pf
# FHWA_Data %>% examine_frequency("B_Miles_Driven")
miles_stl <- FHWA_Data %>% autoSTL(series_name = "B_Miles_Driven", 365/12, 12, 13, show_all_charts)
miles_arima <- miles_stl$arima_ready %>% auto_forecast("B_Miles_Driven", 365/12, show_all_charts)
#miles_arima %>% review_f()
latest_data_df$B_Miles_Driven <- miles_stl$last
forecast_latest_df$B_Miles_Driven <- miles_stl$pf + miles_arima$pf
# US_Gasoline_Production
# fundamentals_data %>% examine_frequency("US_Gasoline_Production")
gas_prod_stl <- fundamentals_data %>% autoSTL("US_Gasoline_Production", 7, 52, 53, show_all_charts)
gas_prod_arima <- gas_prod_stl$arima_ready %>% auto_forecast("US_Gasoline_Production", 7, show_all_charts)
#gas_prod_arima %>% review_f()
latest_data_df$US_Gasoline_Production <- gas_prod_stl$last
forecast_latest_df$US_Gasoline_Production <- gas_prod_stl$pf + gas_prod_arima$pf
# fundamentals_data %>% examine_frequency("US_Crude_NI")
crude_ni_stl <- fundamentals_data %>% autoSTL("US_Crude_NI", 7, 52, 53, show_all_charts)
crude_ni_arima <- crude_ni_stl$arima_ready %>% auto_forecast("US_Crude_NI", 7, show_all_charts)
#crude_ni_arima %>% review_f()
latest_data_df$US_Crude_NI <- crude_ni_stl$last
# model fit with 0 order, omitted from results
forecast_latest_df$US_Crude_NI <- crude_ni_stl$pf
# fundamentals_data %>% examine_frequency("US_Conventional_Stocks_Unblended")
con_stock_stl <- fundamentals_data %>% autoSTL("US_Conventional_Stocks_Unblended", 365/12, 12, 13, show_all_charts)
con_stock_arima <- con_stock_stl$arima_ready %>% auto_forecast("US_Conventional_Stocks_Unblended", 365/12, show_all_charts)
#con_stock_arima %>% review_f()
latest_data_df$US_Conventional_Stocks_Unblended <- con_stock_stl$last
forecast_latest_df$US_Conventional_Stocks_Unblended <-  con_stock_stl$pf + con_stock_arima$pf
#VIF too high, and p-value's too low. Removed
# fundamentals_data %>% examine_frequency("Pct_Operable_Utilization")
# util_stl <- fundamentals_data %>% autoSTL("Pct_Operable_Utilization", 7, 52, 53, show_all_charts)
# util_arima <- util_stl$arima_ready %>% auto_forecast("Pct_Operable_Utilization", 7, show_all_charts)
#util_arima %>% review_f()
# latest_data_df$Pct_Operable_Utilization <- util_stl$last
# forecast_latest_df$Pct_Operable_Utilization <- util_stl$pf + util_arima$pf
# fundamentals_data %>% examine_frequency("US_RBOB_Stocks_Unblended")
# rbob_un_stl <- fundamentals_data %>% autoSTL("US_RBOB_Stocks_Unblended", 365/12, 12, 13, show_all_charts)
# rbob_un_arima <- rbob_un_stl$arima_ready %>% auto_forecast("US_RBOB_Stocks_Unblended", 365/12, show_all_charts)
# #rbob_un_arima %>% review_f()
# latest_data_df$US_RBOB_Stocks_Unblended <- rbob_un_stl$last
# # arima omitted, no fit
# forecast_latest_df$US_RBOB_Stocks_Unblended <- rbob_un_stl$pf
forecast_prediction <- predict(lm_vif_adjusted, forecast_latest_df %>% dplyr::select(-temp)) %>% round(digits = 2)
prediction_last_observed <- predict(lm_vif_adjusted, latest_data_df %>% dplyr::select(-temp)) %>% round(digits = 2)
current_price <- price_data %>%
filter(series == "Crack_Spread") %>%
filter(date == max(date)) %>%
.$value %>%
round(digits = 2)
if(forecast_prediction > current_price){
recommendation <- "long"
inverse <- "short"
} else {
recommendation <- "short"
inverse <- "long"
}
if(forecast_prediction > current_price){
recommendation2 <- "bullish"
} else {
recommendation2 <- "bearish"
}
upside_pot <- abs(current_price - forecast_prediction)
# Show graphics in main body?
show_rc <- TRUE
# Rolling Volatility Crack Spread
rolling_vol <- price_data %>%
dplyr::filter(series == "Crack_Spread") %>%
dplyr::filter(date >= "2022-01-01") %>%
dplyr::mutate(sd20 = slide_dbl(ret_pct, sd, .before = 20, .complete = TRUE)) %>%
dplyr::mutate(sd50 = slide_dbl(ret_pct, sd, .before = 50, .complete = TRUE)) %>%
dplyr::mutate(sd200 = slide_dbl(ret_pct, sd, .before = 200, .complete = TRUE)) %>%
dplyr::mutate(static_sd = sd(ret_pct)) %>%
dplyr::ungroup() %>%
dplyr::select(date, sd20, sd50, sd200, static_sd) %>%
pivot_longer(cols = c("sd20", "sd50", "sd200", "static_sd")) %>%
dplyr::rename(Legend = name) %>%
ggplot(aes(color = Legend)) +
geom_line(aes(x = date, y = value)) +
labs(y = "Volatility of Returns",
x = "Date",
title = "Fig 2: Rolling Volatilities of Crack Spread",
subtitle = "20 day, 50 Day, 200 Day and Average")
if(show_rc){
var_monthly_hist / rolling_vol
}
