---
title: "Gasoline Trading Case Study"
author: "Justin Powley"
format: 
  html:
    self-contained: true
editor: visual
---

```{r, init, message = F, warning = F, error = F, echo = F}
library(tidyquant)
library(tidyverse)
library(RTL)
library(tsibble)
library(feasts)
library(slider)
library(httr)
library(curl)
library(zoo)
library(forecast)
library(corrplot)
library(broom)
library(car)
library(ggfortify)
library(tseries)
library(kableExtra)
library(patchwork)

# Note that sometimes the lack of an API sometimes necessitates loading from excel worksheets.
# This is not the equivalent of modeling in excel, it's instead just another mechanism for us to load data automatically into the model.

# requires Java
# library(xlsx)
library(openxlsx)
library(readxl)

EIA_API <- "UKCNbCB0m8aixfPBshQU3Jdlz2uZsEbG2HebvhHX"
```

```{r, data_import, message = F, warning = F, error = F, echo = F}
# Yahoo Finance
tickers <- c("RB=F", "HO=F", "CL=F", "NG=F")
start_date <- '2018-01-01'
price_data_in <- tq_get(tickers, get = "stock.prices", from = start_date) %>% 
  dplyr::mutate(symbol = case_when(symbol == tickers[1] ~ "RBOB",
                                   symbol == tickers[2] ~ "Heating_Oil",
                                   symbol == tickers[3] ~ "WTI",
                                   symbol == tickers[4] ~ "Natural_Gas"))

eia_tickers <- tribble(~ticker, ~name, "PET.WTTSTUS1.W", "US_Crude_Inventory",
                       "PET.MCRFPUS1.M", "US_Crude_Prod",
                       "NG.NW2_EPG0_SWO_R48_BCF.W", "Lower48NGStorage",
                       "PET.WCRNTUS2.W", "US_Crude_NI",
                       "PET.WRPNTUS2.W", "US_Petrol_Products_NI",
                       "PET.WGFRPUS2.W", "US_Gasoline_Production",
                       "PET.MGFSTUS1.M", "US_Gasoline_Stocks_Blended",
                       "PET.M_EPOBGRR_SAE_NUS_MBBL.M", "US_RBOB_Stocks_Unblended",
                       "PET.MO5ST_NUS_1.M", "US_Conventional_Stocks_Unblended",
                       "PET.WPULEUS3.W", "Pct_Operable_Utilization"
                       )

EIA_Data <- eia2tidy_all(eia_tickers, EIA_API)
# EIA_Data %>% group_by(series) %>% dplyr::mutate(temp = 1, temp = cumsum(temp)) %>% 
#   dplyr::filter(temp == 1) %>% 
#   dplyr::select(-temp)

fred_tickers <- c("IPMINE", "GDP", "TRUCKD11")
fred_data <- tq_get(fred_tickers, get = "economic.data", from = start_date) %>% 
  dplyr::mutate(symbol = case_when(symbol == fred_tickers[1] ~ "Mining_Production_Index",
                                   symbol == fred_tickers[2] ~ "US_GDP",
                                   symbol == fred_tickers[3] ~ "Truck_Tonnage_Index")) %>% 
  dplyr::rename(series = symbol, value = price)

# I'd recommend incorporating Baltic Dry Index (BDI) for shipping by sea. Unfortunately these are metrics which require subscriptions, and as a result aren't included in this report.
```

```{r, preprocessing, warning = F, message = F, error = F, echo = F}
add_ret_metrics <- function(df_in){
  # Accepts a df with columns value, series, date
  # Adds returns and cumulative return metrics for value.
  df_in %>% 
    group_by(series) %>% 
    arrange(series, date) %>% 
    dplyr::mutate(ret_pct = (value-lag(value))/lag(value),
                  ret_log = log(value/lag(value))) %>% 
    tidyr::drop_na() %>% 
    dplyr::mutate(cret_pct = cumprod(ret_pct+1),
                  cret_log = cumsum(ret_log),
                  year_month = yearmonth(date)) %>% 
    return()
}

price_data <- price_data_in %>% 
  dplyr::select(date, series = symbol, value = close, volume)
# rm(price_data_in)

fundamentals_data <- EIA_Data %>% 
  add_ret_metrics()
# rm(EIA_Data)

# 3:2:1 Crack Spread used here. Setting below can be used to adjust the crack spread to other ratios, such as 5:3:2
spread_ratio <- c(3,2,1)

spread_data <- price_data %>% 
  pivot_wider(names_from = series, id_cols = date, values_from = value) %>% 
  dplyr::mutate(Crack_Spread = ((spread_ratio[2]*42*RBOB + 
                                spread_ratio[3]*42*Heating_Oil) - 
                                WTI*spread_ratio[1])/
                                spread_ratio[1])

price_data <- spread_data %>% dplyr::select(date, value = Crack_Spread) %>% 
  dplyr::mutate(series = "Crack_Spread", volume = 0) %>% 
  bind_rows(price_data) %>% 
  add_ret_metrics()


# EIA_Data %>% pivot_wider(names_from = series, id_cols = date, values_from = value) %>% 
#   dplyr::arrange(desc(date))

fred_data <- fred_data %>% add_ret_metrics()
```

```{r, eda, eval = FALSE, echo = F, warning = F}
# Rolling Correlation of RBOB and WTI
spread_data %>% 
  dplyr::mutate(static_cor = cor(WTI,RBOB),
                cor20 = slide2_dbl(RBOB, WTI, cor, .before = 20, .complete = TRUE),
                cor50 = slide2_dbl(RBOB, WTI, cor, .before = 50, .complete = TRUE),
                cor200 = slide2_dbl(RBOB, WTI, cor, .before = 200, .complete = TRUE)
                ) %>% 
  tidyr::drop_na() %>% 
  ggplot() +
  geom_line(aes(x=date, y = cor20, color = "20-Day Corr")) +
  geom_line(aes(x=date, y = cor50, color = "50-Day Corr")) +
  geom_line(aes(x=date, y = cor200, color = "200-Day Corr")) +
  geom_line(aes(x = date, y = static_cor, color = "Static Corr, Full Horizon")) +
  labs(title = "Rolling Correlations, WTI and RBOB",
       subtitle = "How coupled are spreads to the underlying price?",
       x = "Date",
       y = "Correlation",
       color = "Legend") +
  scale_color_manual(values = c("20-Day Corr" = "red", "50-Day Corr" = "blue", "200-Day Corr" = "green", "Static Corr, Full Horizon" = "black"))

# Natural Gas and Spread Price
spread_data %>% 
  ggplot() +
  geom_line(aes(x = date, y = Crack_Spread), color = "red") +
  geom_line(aes(x = date, y = Natural_Gas), color = "black")

# Crude Stocks
fundamentals_data %>% 
  dplyr::filter(series == "US_Crude_Inventory" & date > start_date) %>% 
  ggplot() +
  geom_line(aes(x = date, y = value)) +
  labs(title = "US Crude Stocks", y = "Thousands of Barrels", x = "Date")

# Crude Stocks, PCT Change
fundamentals_data %>% 
  dplyr::filter(series == "US_Crude_Inventory" & date > start_date) %>% 
  ggplot() +
  geom_line(aes(x = date, y = ret_pct)) +
  labs(title = "US Crude Stocks, Changes in Supply", y = "% Change in Supply", x = "Date")

# Plant Utilization versus Crude Inventories
fundamentals_data %>% 
  pivot_wider(names_from = series, id_cols = date, values_from = value) %>% 
  dplyr::select(Pct_Operable_Utilization, US_Crude_Inventory) %>% 
  tidyr::drop_na() %>% 
  cor()

# This graph reveals short term supply/demand shocks as well as a general inflationary pressure which leads to a gradual increase over the course of the time period.
price_data %>% 
  dplyr::filter(series == "RBOB") %>% 
  ggplot() +
  geom_line(aes(x = date, y = value))
```

```{r, Risk1, echo = F, warning = F}
# price_data %>% 
#   dplyr::filter(series == "Crack_Spread" | series == "RBOB" | series == "WTI" | series == "Heating_Oil") %>% 
#   dplyr::mutate(series = case_when(series == "Crack_Spread" ~ "Crack Spread",
#                                    series == "RBOB" ~ "RBOB",
#                                    series == "WTI" ~ "WTI",
#                                    TRUE ~ "Heating Oil")) %>% 
#   dplyr::filter(ret_pct <= 1, ret_pct >= -1) %>% 
#   ggplot() +
#   geom_histogram(aes(x = ret_pct, y = ..count../sum(..count..)), bins = 100) +
#   labs(x = "Percent Return, Daily", y = "Density", title = "Crack Spread Volatility Compared to its Market Dependencies") +
#   facet_grid(series~.)

vol_comparison_1 <- price_data %>% 
  dplyr::filter(series == "Crack_Spread" | series == "RBOB") %>% 
  dplyr::mutate(series = case_when(series == "Crack_Spread" ~ "Crack Spread",
                                   series == "RBOB" ~ "RBOB",
                                   series == "WTI" ~ "WTI",
                                   TRUE ~ "Heating Oil")) %>% 
  dplyr::filter(ret_pct <= 1, ret_pct >= -1) %>% 
  ggplot() +
  geom_histogram(aes(x = ret_pct, y = ..count../sum(..count..)), bins = 100) +
  labs(x = "Percent Return, Daily", y = "Density", title = "Crack Spread Versus Gasoline Volatility") +
  facet_grid(series~.)

# price_data %>% pivot_wider(id_cols = date, names_from = series, values_from = value) %>% 

vol_comparison_2 <- price_data %>% 
  group_by(series) %>% 
  dplyr::summarise(
    `Average Return` = mean(ret_pct),
    `Standard Deviation` = sd(ret_pct),
    .groups = "keep")

# vol_comparison_1
# vol_comparison_2
```

```{r, FHWA, echo = F, warning = F, message = FALSE}
# Pulling distance traveled estimates from FHWA
# https://www.fhwa.dot.gov/policyinformation/travel_monitoring/tvt.cfm
# Potential to create a fully automated workflow. For now, manually link the reports for the time period of interest.

# Must be newest to oldest
links <- c(FHWA2024 = "https://www.fhwa.dot.gov/policyinformation/travel_monitoring/24juntvt/24juntvt.xlsx",
           FHWA2023 = "https://www.fhwa.dot.gov/policyinformation/travel_monitoring/23dectvt/23dectvt.xlsx",
           FHWA2022 = "https://www.fhwa.dot.gov/policyinformation/travel_monitoring/22dectvt/22dectvt.xlsx",
           FHWA2021 = "https://www.fhwa.dot.gov/policyinformation/travel_monitoring/21dectvt/21dectvt.xlsx",
           FHWA2020 = "https://www.fhwa.dot.gov/policyinformation/travel_monitoring/20dectvt/20dectvt.xls",
           FHWA2019 = "https://www.fhwa.dot.gov/policyinformation/travel_monitoring/19dectvt/19dectvt.xls")

month_tags <- c("JAN", "FEB", "MAR", "APR", "MAY", "JUN", "JUL", "AUG", "SEP", "OCT", "NOV", "DEC")

clean_FHWA <- function(df_in){
  
  df_in_year <- paste(df_in %>% head(1), collapse = " ") %>% str_extract(pattern = "[0-9][0-9][0-9][0-9]")
  
  df_in %>% 
    tail(1) %>% 
    # There has to be a way to do this better, but I just found out xlsx package is java dependent, so we're using a simpler       solution here.
    dplyr::mutate(JAN = as.numeric(JAN),
                  FEB = as.numeric(FEB),
                  MAR = as.numeric(MAR),
                  APR = as.numeric(APR),
                  MAY = as.numeric(MAY),
                  JUN = as.numeric(JUN),
                  JUL = as.numeric(JUL),
                  AUG = as.numeric(AUG),
                  SEP = as.numeric(SEP),
                  OCT = as.numeric(OCT),
                  NOV = as.numeric(NOV),
                  DEC = as.numeric(DEC),) %>% 
    pivot_longer(names_to = "month", cols = month_tags) %>% 
    dplyr::select(month, value) %>% 
    dplyr::mutate(year = df_in_year) %>% 
    drop_na() %>% 
    return()
}

for(i in 1:length(links)){
  
  if(str_detect(links[i], pattern = ".xlsx")){
    ext <- ".xlsx"
  } else {
    ext <- ".xls"
  }
  
  path <- paste0(names(links)[i], ext)
  curl_download(links[i], path)

  # I am hesitant to use specific row indexes here, due to issues which can arise if the template changes. In   a full     implementation I'd add checks for keywords in certain rows.cells to ensure that necessary updates and   mismatches aren't missed.
  
  # Java dependency fix called for quick adjustments prior to delivery
  if(ext == ".xlsx"){
  df1 <- openxlsx::read.xlsx(path, sheet = "Page 3", rows = c(3,4,12))
  df2 <- openxlsx::read.xlsx(path, sheet = "Page 3", rows = c(3,13,20))
  } else {
    
  df1 <- read_excel(path, sheet = "Page 3")
  df1 <- df1[c(2,3,11),] %>% 
    dplyr::select(c(-1,-2,-3,-16))
  colnames(df1) <- df1[1, ]
  df1 <- df1 %>% tail(-1)
  
  df2 <- read_excel(path, sheet = "Page 3")
  df2 <- df2[c(2,12,19),] %>% 
    dplyr::select(c(-1,-2,-3,-16))
  colnames(df2) <- df2[1, ]
  df2 <- df2 %>% tail(-1)
  }

  if(i == 1){
    FHWA_Data <- df2 %>% clean_FHWA()
  }
  FHWA_Data <- bind_rows(df1 %>% clean_FHWA(), FHWA_Data)
}

FHWA_Data <- FHWA_Data %>% 
  dplyr::mutate(date = yearmonth(paste(month, year)),
                series = "B_Miles_Driven") %>% 
  dplyr::select(series, date, value) %>% 
  dplyr::mutate(date = as.Date(date),
                value = as.numeric(value)) %>% 
  add_ret_metrics()
  
# This uncovers for us some of the seasonal demand for fuel.
seasonal_demand_1 <- FHWA_Data %>% 
  arrange(date) %>% 
  ggplot() +
  geom_line(aes(x = date, y = value)) +
  labs(title = "Seasonal patterns in road travel, using FHWA Data",
       x = "Date", y = "Billions of Miles Traveled")

# seasonal_demand_1
```

```{r test_space_1, eval = FALSE, echo = FALSE}
FHWA_Data %>% s_graph("B_Miles_Driven")
```


```{r, s_graph, echo = F, warning = F}
# Re-visualizing data every time I wanted to add another data source was getting inefficient;
# Created a generalized line graph function
s_graph <- function(data, series_name, start_date_in = start_date){
  data %>% 
    dplyr::filter(series == series_name) %>% 
    dplyr::filter(date >= start_date_in) %>% 
    ggplot() +
    geom_line(aes(y = value, x = date)) + 
    labs(title = series_name) %>% 
    return()
}
```

```{r, more eda, eval = F, echo = F, warning = F}
# Fred
for(s in fred_data$series %>% unique()){
 s_graph(fred_data, s) %>% print()
}

# Price Data
for(s in price_data$series %>% unique()){
 s_graph(price_data, s) %>% print()
}

# Fundamentals Data, EIA
for(s in fundamentals_data$series %>% unique()){
 s_graph(fundamentals_data, s) %>% print()
}

for(s in FHWA_Data$series %>% unique()){
 s_graph(FHWA_Data, s) %>% print()
}
# Notes: Some indicators here track with GDP (Note the strong uptrend. To incorporate these, we may consider normalizing each index to start at 1, and then net the difference between GDP and the other indicator, making a spread)
```

```{r, aggregation, model_prep, echo = F, warning = F}
# Note data misalignment. Reports may publish on non-trading days and therefore we need to approximate lining up time periods. For now, we will use the weekly close, and then match the week a report releases on.

year_week_conversion <- function(df_in){
  df_in %>% 
    dplyr::mutate(year_week = paste0(year(date),"-",week(date) %>% sprintf("%02d", .))) %>% 
    arrange(series, date) %>% 
    group_by(series, year_week) %>% 
    dplyr::mutate(temp = 1, temp = cumsum(temp)) %>% 
    dplyr::filter(temp == max(temp)) %>% 
    return()
}

aggregate <- price_data %>% year_week_conversion()

aggregate <- fundamentals_data %>% year_week_conversion() %>% bind_rows(aggregate)

aggregate <- fred_data %>% year_week_conversion() %>% bind_rows(aggregate)

aggregate <- FHWA_Data %>% year_week_conversion() %>% bind_rows(aggregate)  

wide_agg <- aggregate %>% 
  dplyr::filter(date >= start_date) %>% 
  pivot_wider(id_cols = year_week, names_from = "series", values_from = value) %>% 
  arrange(year_week)

# Creates a re-key using the last date matching a year-week
date_key <- price_data %>% 
  year_week_conversion() %>% 
  ungroup() %>% 
  dplyr::select(date, year_week) %>% 
  arrange(date) %>% 
  group_by(year_week) %>% 
  dplyr::mutate(temp = 1, temp = cumsum(temp)) %>% 
  dplyr::filter(temp == max(temp)) %>% 
  dplyr::select(-temp)

wide_agg <- wide_agg %>% left_join(date_key, by = "year_week")

# Interpolation, Leaving non-updated values the same as previous.
# Opted for linear interpolation due to a lack of high frequency data. Later, we will conduct additional decomposition and ARIMA models for those which exhibit more structured behavior.

# Interpolated
# wide_agg %>% 
#   ungroup() %>% 
#   dplyr::mutate(B_Miles_Driven = na.interp(B_Miles_Driven),
#                 Mining_Production_Index = na.interp(Mining_Production_Index),
#                 Truck_Tonnage_Index = na.interp(Truck_Tonnage_Index),
#                 US_GDP = na.interp(US_GDP),
#                 US_Conventional_Stocks_Unblended = na.interp(US_Conventional_Stocks_Unblended),
#                 US_Crude_Prod = na.interp(US_Crude_Prod),
#                 US_Gasoline_Stocks_Blended = na.interp(US_Gasoline_Stocks_Blended),
#                 US_RBOB_Stocks_Unblended = na.interp(US_RBOB_Stocks_Unblended)
#                 ) %>% 
#   dplyr::select(-date, -year_week, -Heating_Oil, - WTI, -RBOB, -Lower48NGStorage, - Natural_Gas) %>% 
#   drop_na() %>% 
#   cor() %>% 
#   # Credit to ChatGPT for improved formatting on this CorrPlot
#   corrplot( 
#          method = "color",
#          col = colorRampPalette(c("#6D9EC1", "white", "#E46726"))(200),
#          type = "upper",
#          order = "hclust",
#          addCoef.col = "black",
#          tl.col = "black",
#          tl.srt = 45,
#          diag = FALSE,
#          number.cex = 0.6,
#          addshade = "positive",
#          cl.lim = c(-1, 1),
#          mar = c(0, 0, 1, 0)
# )

# Non Interpolated
uninterpolated_data_w_date <- wide_agg %>% 
  ungroup() %>% 
  dplyr::select(-year_week, -Heating_Oil, - WTI, -RBOB, -Lower48NGStorage, - Natural_Gas, - US_GDP) %>%
  drop_na() %>% 
  dplyr::mutate(t = 1, t = cumsum(t))

uninterpolated_data <- uninterpolated_data_w_date %>% dplyr::select(-date, -t)

# uninterpolated_data %>% 
#   cor() %>% 
#   corrplot( 
#          method = "color",
#          col = colorRampPalette(c("#6D9EC1", "white", "#E46726"))(200),
#          type = "upper",
#          order = "hclust",
#          addCoef.col = "black",
#          tl.col = "black",
#          tl.srt = 45,
#          diag = FALSE,
#          number.cex = 0.6,
#          addshade = "positive",
#          cl.lim = c(-1, 1),
#          mar = c(0, 0, 1, 0)
# )
# 
# uninterpolated_data
```

```{r, demonstrational_graphics, echo = F, warning = F}
driver_graph_1 <- fred_data %>% dplyr::filter(series == "Truck_Tonnage_Index") %>% 
  dplyr::mutate(ret_pct = ret_pct * 10,
                cret_pct = cumprod(1+ret_pct),
                cret_pct = (cret_pct+1)*5-5) %>% 
  bind_rows(price_data %>% dplyr::filter(series == "Crack_Spread" | series == "Heating_Oil")) %>% 
  dplyr::filter(date > "2021-01-01") %>% 
  ggplot() +
  geom_line(aes(x = date, y = cret_pct, color = series)) +
  facet_grid(series~.) +
  labs(y = "Index, Range Adjusted",
       title = "Truck Tonnage Influence on Crack Spreads & Heating Oil",
       x = "Date")

# wide_agg %>% 
#   ungroup() %>% 
#   drop_na() %>% 
#   dplyr::select(Truck_Tonnage_Index, Crack_Spread, Heating_Oil) %>% 
#   cor()

# driver_graph_1
```

```{r, linear model, echo = F, warning = F}
# lm1 <- uninterpolated_data %>% lm(Crack_Spread ~ ., data = .)
# lm1 %>% broom::tidy() %>% 
#   arrange(p.value)
# lm1 %>% broom::glance()
# # Not a bad model for pricing predictions, need to address VIF most likely
# 
# lm1 %>% vif() %>% as.data.frame() %>% 
#   dplyr::rename(VIF = 1) %>% 
#   arrange(desc(VIF))
# 
# corr_slice <- function(df_in, series_select){
# df_in %>% 
#   cor() %>% 
#   as.data.frame() %>% 
#   rownames_to_column(var = "series") %>% 
#   dplyr::filter(series == series_select) %>% 
#   t() %>% 
#   as.data.frame() %>% 
#   rename(Correlation = 1) %>% 
#   tail(-1) %>% 
#   arrange(desc(Correlation)) %>% 
#   tail(-1) %>% 
#   return()
# }
# 
# uninterpolated_data %>% corr_slice("Mining_Production_Index")
# uninterpolated_data %>% corr_slice("US_Crude_Prod")

# Dropping Mining Production Index due to high VIF. Because it intersects highly with US_Crude_Prod, our next high VIF, we remodel and reassess.

# uninterpolated_data %>% 
#   dplyr::select(-Mining_Production_Index) %>% 
#   lm(Crack_Spread ~ ., data = .) %>% 
#   vif() %>% 
#   as.data.frame() %>% 
#   dplyr::rename(VIF = 1) %>% 
#   arrange(desc(VIF))

# Crude Inventory is High, however it is a core market driver in our model, and has higher data frequency. As such, we will assess other factors and remove the ones with highest overlap.

# uninterpolated_data %>% 
#   dplyr::select(-Mining_Production_Index) %>% 
#   corr_slice("US_Crude_Inventory")

# We notice here that the highest correlations are to gasoline stocks and petrol imports/exports. We should be okay to drop both of these, since blended gasoline stock is a more compressed portion of the market (smaller number compared to RBOB and Conventional blending formula) and we notice lower correlations in those categories.

# Net imports makes sense. As stocks rise, some companies may import gasoline/diesel to meet rushed demand if they are slightly behind in their refined product inventories, and thus we're better off simply keeping the Crude Inventory Level to reduce the VIF.

# Time to test VIF once more:
data_vif_adj <- uninterpolated_data %>% 
  dplyr::select(-Mining_Production_Index,
                -US_Gasoline_Stocks_Blended,
                -US_Petrol_Products_NI,
                -Pct_Operable_Utilization,
                -US_RBOB_Stocks_Unblended)

lm_vif_adjusted <- data_vif_adj %>% 
  lm(Crack_Spread ~ ., data = .)

final_reg_vif <- lm_vif_adjusted %>% 
  vif() %>% 
  as.data.frame() %>% 
  dplyr::rename(VIF = 1) %>% 
  arrange(desc(VIF))
  
final_reg_p <- lm_vif_adjusted %>% broom::glance()

final_reg_c <- lm_vif_adjusted %>% broom::tidy() %>% 
  arrange(p.value)

# This model appears to be pretty decent for regressions assumptions. Normal Q-Q looks nice, some slight curvature in Residuals vs Fitted, but otherwise this model seems pretty valid.

final_reg_qq <- lm_vif_adjusted %>% autoplot()
data_vif_adj$prediction <- lm_vif_adjusted %>% predict()

data_with_prediction <- data_vif_adj %>% 
  dplyr::mutate(t = 1, t = cumsum(t)) %>% 
  dplyr::left_join(uninterpolated_data_w_date %>% dplyr::select(t,date), by = "t")
  
final_reg_graph <- data_with_prediction %>% 
  dplyr::select(date, prediction, Crack_Spread) %>% 
  dplyr::rename(Prediction = prediction) %>% 
  pivot_longer(names_to = "Legend", cols = c("Prediction", "Crack_Spread")) %>% 
  ggplot(aes(color = Legend)) +
  geom_line(aes(x=date, y=value)) +
  geom_line(aes(x=date, y=value)) +
  labs(title = "Predicting Crack Spread with Fundamental Drivers",
       x = "Date",
       y = "Price",
       legend = "Legend")



# Now for the final step, we need to extrapolate our drivers to today, to predict the crack spread price and compare to the current market price. We can do this in a variety of ways. 

# 1. For random walks, we will hold our assumption constant from the last observed market price.
# 2. For data with more structure, we can do an additive or STL decomposition to remove seasonality and trend, and do an ARMA model on the residuals.

# Note: this is the step where having higher frequency data, through subscriptions and data provider services really pays off. Reducing the extrapolation period can significantly improve model accuracy resulting in a stronger competitive edge.


# Outputs for Appendix:

# final_reg_p
# final_reg_c
# final_reg_qq
# final_reg_graph
# final_reg_vif
```

```{r, echo = F, warning = F}
latest_data_df <- data.frame(temp = NA)
forecast_latest_df <- data.frame(temp = NA)
```

```{r, generalized forecasting function, echo = F, warning = F}
# Let's standardize our forecasting process for the other 8 variables

# Step 1: Filter Data for series, cleanup
# Step 2: Prep TS
# Step 3: Initial ACF/PACF
# Step 4: Fit Model
# Step 5: Summarize and Q-Stat
# Step 6: Residual ACF/PACF
# Step 7: Interpolate Results to Match Current Date    

auto_forecast <- function(data_in, series_name, period_days, show_plot = F){
# Plots and Model Fitting:

output_list <- list()

data_in <- data_in %>% 
  ungroup() %>% 
  dplyr::filter(series == series_name) %>% 
  dplyr::filter(date >= start_date) %>% 
  arrange(date)

if(show_plot){
  line_plot <- data_in %>% 
    ggplot() +
    geom_line(aes(x = date, y = value)) +
    labs(title = series_name)
  
  print(line_plot)
}

ts_in <- data_in %>% 
  dplyr::select(value) %>% 
  ts()

acf1 <- ts_in %>% acf(main = paste("ACF of", series_name), plot = show_plot)
pacf1 <- ts_in %>% pacf(main = paste("PACF of", series_name), plot = show_plot)

fit <- auto.arima(ts_in)
summary(fit)

acf2 <- fit$residuals %>% acf(main = paste("ACF of", series_name, "Residuals"), plot = show_plot)
pacf2 <- fit$residuals %>% pacf(main = paste("PACF of", series_name, "Residuals"), plot = show_plot)

box <- Box.test(fit$residuals, type = "Ljung-Box")

# Horizon, Forecasting and Interpolation

last_date <- data_in %>%
  filter(date == max(date)) %>% 
  .$date

time_diff <- as.numeric(Sys.Date() - last_date)

last_point <- data_in %>%
  filter(date == max(date)) %>% 
  .$value

forecast_length <- ceiling(time_diff/period_days)

#Debug Section, Contains Fixed Assignments. Comment out if not in use

#forecast_length <- 2

# Debug End

if(forecast_length > 0){
  forecast_arima <- forecast(fit, h = forecast_length) %>% as.data.frame()
  
  if(forecast_length == 1){
    #print(1)
    ratio <- time_diff/period_days
    forecast_out <- ratio*forecast_arima$`Point Forecast` + (1-ratio)*last_point
  } else {
    #print(2)
    ratio <- (time_diff/period_days) - (floor(time_diff/period_days))
    forecast_out <- ratio*forecast_arima$`Point Forecast`[forecast_length] + 
      (1-ratio)*forecast_arima$`Point Forecast`[forecast_length-1]
  }
  
} else {
  #print(0)
  forecast_out <- last_point
}

output_list$pf <- forecast_out
output_list$last <- last_point
output_list$data_acf <- acf1
output_list$data_pacf <- pacf1
output_list$res_acf <- acf2
output_list$res_pacf <- pacf2
output_list$q_stat <- box$p.value
output_list$order <- arimaorder(fit)
output_list$fit <- fit

return(output_list)
}

review_f <- function(list_in){
  print(list_in$order)
  print(list_in$q_stat)
}

examine_frequency <- function(df_in, series_name){
  df_in %>% 
    ungroup() %>% 
    arrange(date) %>% 
    filter(series == series_name) %>% 
    return()
}
```

```{r, autoSTL function, echo = F, warning = F}

autoSTL <- function(data_in, series_name, period_days, freq, window_size, show_plot = F, date_filter = start_date){
  
  output_list <- list()

  # Data Prep
  
  ts_in <- data_in %>% 
    dplyr::filter(date >= date_filter) %>% 
    dplyr::filter(series == series_name) %>% 
    ungroup() %>% 
    arrange(date) %>% 
    dplyr::select(value) %>% 
    .$value %>% 
    as.vector() %>% 
    ts(frequency = freq)
  
  data_in <- data_in %>% 
    ungroup() %>% 
    dplyr::filter(series == series_name) %>% 
    dplyr::filter(date >= start_date) %>% 
    arrange(date)
  
   if(show_plot){
   line_plot <- data_in %>% 
     ggplot() +
     geom_line(aes(x = date, y = value)) +
     labs(title = series_name, x = "Date", y = "Value")
   
   print(line_plot)
   }
  
  # Fitting
  
  fit <- stl(ts_in, s.window = window_size)
  
  if(show_plot){
    plot(fit)
  }
  
  # Horizon and Interpolation Prep
  
  last_date <- data_in %>%
  filter(date == max(date)) %>% 
  .$date

  time_diff <- as.numeric(Sys.Date() - last_date)

  last_point <- data_in %>%
    filter(date == max(date)) %>% 
    .$value

  forecast_length <- ceiling(time_diff/period_days)
  
  #Debug Section, Contains Fixed Assignments. Comment out if not in use

  # forecast_length <- 0

  # Debug End
  
  # Forecast and Interpolation

  if(forecast_length > 0){
  forecast_stl <- forecast(fit, h = forecast_length) %>% as.data.frame()
  
    if(forecast_length == 1){
    #print(1)
      ratio <- time_diff/period_days
      forecast_out <- ratio*forecast_stl$`Point Forecast` + (1-ratio)*last_point
    } else {
    #print(2)
      ratio <- (time_diff/period_days) - (floor(time_diff/period_days))
      forecast_out <- ratio*forecast_stl$`Point Forecast`[forecast_length] + 
        (1-ratio)*forecast_stl$`Point Forecast`[forecast_length-1]
    }
  
  } else {
  #print(0)
    forecast_out <- last_point
  }
  
  output_list$pf <- forecast_out
  output_list$last <- last_point
  output_list$fit <- fit
  
  res_df <- fit$time.series[, "remainder"] %>% as.data.frame()
  
  output_list$arima_ready <- cbind(data_in, res_df) %>% dplyr::mutate(value = x)
  
  return(output_list)
}

```

```{r, Forecasting, echo = F, warning = F}
show_all_charts <- FALSE

#Appears to be a randnom walk, using direct ARIMA approach to integrate the model.
crude_inv_model <- auto_forecast(fundamentals_data, "US_Crude_Inventory", 7, show_all_charts)
latest_data_df$US_Crude_Inventory <- crude_inv_model$last
forecast_latest_df$US_Crude_Inventory <- crude_inv_model$pf

tonnage_model <- auto_forecast(fred_data, "Truck_Tonnage_Index", 365/12, show_all_charts)
#tonnage_model %>% review_f()
latest_data_df$Truck_Tonnage_Index <- tonnage_model$last
forecast_latest_df$Truck_Tonnage_Index <- tonnage_model$pf
  
crude_prod_stl <- fundamentals_data %>% autoSTL(series_name = "US_Crude_Prod", 365/12, 12, 13, show_all_charts)
crude_prod_arma <- crude_prod_stl$arima_ready %>% auto_forecast("US_Crude_Prod", 365/12, show_all_charts)
#crude_prod_arma %>% review_f()
latest_data_df$US_Crude_Prod <- crude_prod_stl$last
forecast_latest_df$US_Crude_Prod <- crude_prod_stl$pf + crude_prod_arma$pf

# FHWA_Data %>% examine_frequency("B_Miles_Driven")
miles_stl <- FHWA_Data %>% autoSTL(series_name = "B_Miles_Driven", 365/12, 12, 13, show_all_charts)
miles_arima <- miles_stl$arima_ready %>% auto_forecast("B_Miles_Driven", 365/12, show_all_charts)
#miles_arima %>% review_f()
latest_data_df$B_Miles_Driven <- miles_stl$last
forecast_latest_df$B_Miles_Driven <- miles_stl$pf + miles_arima$pf

# US_Gasoline_Production
# fundamentals_data %>% examine_frequency("US_Gasoline_Production")
gas_prod_stl <- fundamentals_data %>% autoSTL("US_Gasoline_Production", 7, 52, 53, show_all_charts)
gas_prod_arima <- gas_prod_stl$arima_ready %>% auto_forecast("US_Gasoline_Production", 7, show_all_charts)
#gas_prod_arima %>% review_f()
latest_data_df$US_Gasoline_Production <- gas_prod_stl$last
forecast_latest_df$US_Gasoline_Production <- gas_prod_stl$pf + gas_prod_arima$pf

# fundamentals_data %>% examine_frequency("US_Crude_NI")
crude_ni_stl <- fundamentals_data %>% autoSTL("US_Crude_NI", 7, 52, 53, show_all_charts)
crude_ni_arima <- crude_ni_stl$arima_ready %>% auto_forecast("US_Crude_NI", 7, show_all_charts)
#crude_ni_arima %>% review_f()
latest_data_df$US_Crude_NI <- crude_ni_stl$last
# model fit with 0 order, omitted from results
forecast_latest_df$US_Crude_NI <- crude_ni_stl$pf

# fundamentals_data %>% examine_frequency("US_Conventional_Stocks_Unblended")
con_stock_stl <- fundamentals_data %>% autoSTL("US_Conventional_Stocks_Unblended", 365/12, 12, 13, show_all_charts)
con_stock_arima <- con_stock_stl$arima_ready %>% auto_forecast("US_Conventional_Stocks_Unblended", 365/12, show_all_charts)
#con_stock_arima %>% review_f()
latest_data_df$US_Conventional_Stocks_Unblended <- con_stock_stl$last
forecast_latest_df$US_Conventional_Stocks_Unblended <-  con_stock_stl$pf + con_stock_arima$pf

#VIF too high, and p-value's too low. Removed

# fundamentals_data %>% examine_frequency("Pct_Operable_Utilization")
# util_stl <- fundamentals_data %>% autoSTL("Pct_Operable_Utilization", 7, 52, 53, show_all_charts)
# util_arima <- util_stl$arima_ready %>% auto_forecast("Pct_Operable_Utilization", 7, show_all_charts)
#util_arima %>% review_f()
# latest_data_df$Pct_Operable_Utilization <- util_stl$last
# forecast_latest_df$Pct_Operable_Utilization <- util_stl$pf + util_arima$pf

# fundamentals_data %>% examine_frequency("US_RBOB_Stocks_Unblended")
# rbob_un_stl <- fundamentals_data %>% autoSTL("US_RBOB_Stocks_Unblended", 365/12, 12, 13, show_all_charts)
# rbob_un_arima <- rbob_un_stl$arima_ready %>% auto_forecast("US_RBOB_Stocks_Unblended", 365/12, show_all_charts)
# #rbob_un_arima %>% review_f()
# latest_data_df$US_RBOB_Stocks_Unblended <- rbob_un_stl$last
# # arima omitted, no fit
# forecast_latest_df$US_RBOB_Stocks_Unblended <- rbob_un_stl$pf
  
```

```{r, model recommendation, echo = F, warning = F}
forecast_prediction <- predict(lm_vif_adjusted, forecast_latest_df %>% dplyr::select(-temp)) %>% round(digits = 2)

prediction_last_observed <- predict(lm_vif_adjusted, latest_data_df %>% dplyr::select(-temp)) %>% round(digits = 2)

current_price <- price_data %>%
  filter(series == "Crack_Spread") %>% 
  filter(date == max(date)) %>% 
  .$value %>% 
  round(digits = 2)

if(forecast_prediction > current_price){
  recommendation <- "long"
  inverse <- "short"
} else {
  recommendation <- "short"
  inverse <- "long"
}



if(forecast_prediction > current_price){
  recommendation2 <- "bullish"
} else {
  recommendation2 <- "bearish"
}

upside_pot <- abs(current_price - forecast_prediction)
```

## Recommendation

```{r, echo = F}
# Show graphics in main body?
show_rc <- TRUE
```

The 3-2-1 crack spread looks **`r recommendation2`** based on market fundamentals. Our model predicts a crack spread of **`r forecast_prediction` using forecasted** values and **`r prediction_last_observed` using the most recently observable** data points. Based on the most recent 3-2-1 crack spread **price of `r current_price`**, a **`r recommendation` position** in the crack spread is recommended.

## Trading Strategy

Enter `r recommendation` into 2 gasoline and 1 heating oil futures contract, and then enter `r inverse` into 3 crude oil contracts. Note that crude oil contract volumes are 1000 barrels on the CME and RBOB and Heating Oil volumes are 42,000 gallons (1000 Barrels). Alternatively, enter `r recommendation` into a 3-2-1 Crack Spread Future.

## Rationale

```{r, echo = FALSE, fig.width=8, fig.height=2}
if(show_rc){
  final_reg_graph
}
```

The prediction is driven by a regression model (shown above) consisting of **7 market drivers**, with sources from the Energy Information Administration, Federal Reserve Economic Data, Yahoo Finance, and Federal Highway Administration. Since the **regression model** makes use of aligned monthly data points, without interpolation, it's better used when entering longer time frame positions with a clear stop loss and exit strategy. This model attempts to predict current prices using current fundamental market levels. We choose which position to enter based on whether our prediction is higher or lower than the current price, assuming that our indicator and the market will at some point converge.

Because our data is lagging (not live), forecasting models are developed for each market driver. **STL decomposition** is used to capture larger structural patterns such as seasonality and trend, and **ARIMA models** are then used to capture remaining dependency in the residuals. This allows us to predict today's fundamental levels using previous levels, seasonality and trend. We also predict price using the last observable data points, as a way of measuring the influence of our forecasts on the decision.

## Fundamental Market Drivers

Data was selected by correlation with the crack spread prices and by data frequency/quality:

::: {.columns}
:::: {.column width="50%" style="padding: 0; margin: 0;"}
<span style="font-weight: bold; font-size: 90%;">Demand Side (Bullish Influence) Factors:</span>
<ul style="margin: 0; padding-left: 1.2em; font-size: 90%;">
  <li>Truck Tonnage</li>
  <li>Billion Miles Driven</li>
</ul>
::::

:::: {.column width="50%" style="padding: 0; margin: 0;"}
<span style="font-weight: bold; font-size: 90%;">Supply Side (Bearish Influence) Factors:</span>
<ul style="margin: 0; padding-left: 1.2em; font-size: 90%;">
<li>US Crude Inventory</li>
<li>US Crude Production</li>
<li>US Gasoline Production</li>
<li>US Crude Net Imports</li>
<li>Gasoline Stocks, Conventional</li>
</ul>
::::
:::

## Risk/Reward

Model risk arises from structural changes to fundamental-price equilibrium. When this arises, it can be solved by aging out part of the training data for the regression model.

Sometimes our market drivers will instead respond to the current crack spread. This results in our prediction moving instead of the market price. Assuming no reversals in dependency, our model forecasts an **upside potential of `r upside_pot` per barrel**, however this is likely overstated as our spread price and prediction may converge.

```{r, echo = FALSE, warning = F}
# VAR Metrics
var_monthly <- price_data %>% 
  dplyr::filter(series == "Crack_Spread") %>% 
  dplyr::mutate(monthly_cret = slide_dbl(ret_pct+1, prod, .before = 20, .complete = TRUE),
                monthly_cret = monthly_cret - 1) %>% 
  drop_na()

var_monthly_hist <- var_monthly %>% 
  dplyr::filter(abs(monthly_cret) < 2) %>% 
  ggplot() +
  geom_histogram(aes(x = monthly_cret, y = ..count../sum(..count..)), bins = 100) +
  labs(title = "Fig 1: Monthly Returns Crack Spread",
       subtitle = "Sliding Window",
       x = "Monthly Return",
       y = "Density")

threshold <- 0.05

if(recommendation == "long"){
  VAR <- var_monthly %>% 
    dplyr::filter(monthly_cret <= quantile(monthly_cret, threshold)) %>% 
    dplyr::filter(monthly_cret == max(monthly_cret)) %>% 
    .$monthly_cret %>% 
    round(digits = 3)
} else {
  VAR <- var_monthly %>% 
    dplyr::filter(monthly_cret >= quantile(monthly_cret, (1-threshold))) %>% 
    dplyr::filter(monthly_cret == min(monthly_cret)) %>% 
    .$monthly_cret %>% 
    round(digits = 3)
  VAR <- VAR*-1
}
```

We use Value at Risk because it gives us a less biased risk estimate independent from our model. The **5% Value at Risk on 1 month positions in the crack spread is `r VAR`**. Our best outcome in the worst 5% of past cases is `r VAR` on a 1 month position. Tail risk is more extreme for short positions than long positions (Fig 1). Liquidity risk (the risk of closing out early) can be better assessed using rolling volatilities (Fig 2). Note that rolling 20 and 50 day volatilities are currently low, however note the tendency for 20 day volatilities to spike above average on occasion:

```{r, echo = FALSE, warning = F, fig.width=8, fig.height=4}
# Rolling Volatility Crack Spread
rolling_vol <- price_data %>% 
  dplyr::filter(series == "Crack_Spread") %>% 
  dplyr::filter(date >= "2022-01-01") %>% 
  dplyr::mutate(sd20 = slide_dbl(ret_pct, sd, .before = 20, .complete = TRUE)) %>% 
  dplyr::mutate(sd50 = slide_dbl(ret_pct, sd, .before = 50, .complete = TRUE)) %>% 
  dplyr::mutate(sd200 = slide_dbl(ret_pct, sd, .before = 200, .complete = TRUE)) %>% 
  dplyr::mutate(static_sd = sd(ret_pct)) %>% 
  dplyr::ungroup() %>% 
  dplyr::select(date, sd20, sd50, sd200, static_sd) %>% 
  pivot_longer(cols = c("sd20", "sd50", "sd200", "static_sd")) %>% 
  dplyr::rename(Legend = name) %>% 
  ggplot(aes(color = Legend)) +
  geom_line(aes(x = date, y = value)) +
  labs(y = "Volatility of Returns",
       x = "Date",
       title = "Fig 2: Rolling Volatilities of Crack Spread",
       subtitle = "20 day, 50 Day, 200 Day and Average")

if(show_rc){
  var_monthly_hist / rolling_vol
}
```

## Appendix

### Regression, Primary Model

```{r, echo = FALSE}
final_reg_graph
final_reg_p %>% dplyr::select(R_Squared = r.squared, 
                              Adj_R_Squared = adj.r.squared, 
                              SD = sigma, 
                              P_Val = p.value, 
                              Degrees_Freedom = df, 
                              AIC, 
                              BIC) %>% kable()
final_reg_c %>% 
  dplyr::select(Market_Driver = term, Coefficient = estimate, Standard_Error = std.error, P_Val = p.value) %>% 
  kable()
```


### Additional Model Assessment

```{r, echo = F}
final_reg_qq
final_reg_vif %>% kable()
```

### How does each driver factor into forecasted price?

```{r, echo = FALSE}
forecast_latest_df$`(Intercept)` <- 1
df1 <- forecast_latest_df %>% t() %>% as.data.frame() %>% rownames_to_column(var = "Market_Driver") %>% 
  drop_na()
df2 <- final_reg_c %>% 
  dplyr::select(Market_Driver = term, Coefficient = estimate, Standard_Error = std.error, P_Val = p.value)
df3 <- left_join(df2,df1,by="Market_Driver") %>% dplyr::mutate(Value = Coefficient * V1) %>% 
  dplyr::select(Market_Driver, Value)

df3 %>% kable()
df3 %>% summarise(Estimate = sum(Value)) %>% kable()

```

### Risk Assessment Visuals

```{r, echo = F, warning = F, message = F, fig.width=8, fig.height=5}
vol_comparison_1 + labs(subtitle = "Crack Spread is more Volatile than RBOB")
var_monthly_hist / rolling_vol
```

### Samples of ARIMA and STL Models

*Charts counts for STL and ARMA models exceed 45.* For brevity, only sample visualizations and stats used to assess model fit are included below. These examples showcase the diagnostic tools used to fit each model. For a more detailed view of each model, go to the *Forecasting* code block and set show_all_charts <- True. Note that these charts will appear at the front of the report when rendered, since they are earlier in the workflow.

#### Sample ARIMA Model

The charts below show the model fitting workflow in order:

* The Raw Data Graphed, to visually assess for structure.
* The ACF and PACF of the raw data.
* The ACF and PACF of Residuals
* The model order to assess structure type.
* The Q-Stat, to assess whether final error resembles white noise.

##### ARIMA Model on US Crude Inventories | Random Walk

```{r, echo = F, fig.width=8, fig.height=4}
crude_inv_model <- auto_forecast(fundamentals_data, "US_Crude_Inventory", 7, F)

fundamentals_data %>% s_graph("US_Crude_Inventory") + labs(title = "US Crude Inventory", y = "Thousand Barrels", x = "Date")
crude_inv_model$data_acf %>% autoplot() + labs(title = "ACF | US Crude Inventory | Raw Data")
crude_inv_model$data_pacf %>% autoplot() + labs(title = "PACF | US Crude Inventory | Raw Data")
crude_inv_model$res_acf %>% autoplot() + labs(title = "ACF | US Crude Inventory | Residuals")
crude_inv_model$res_pacf %>% autoplot() + labs(title = "PACF | US Crude Inventory | Residuals")
q_stat1 <- crude_inv_model$q_stat
order1 <- crude_inv_model$order
```
The **model order (p,d,q) is `r order1`** and the **q-stat p-value is `r q_stat1`.** Larger p-values are better in this case. The null represents white noise and low likelihood of further model improvement through structural capture.

#### Sample STL Model | Billions of Miles Driven

Below is an example of using STL to capture seasonal and trend components in more regular data. The function developed also returns an ARIMA ready data set of residuals, to generate further analysis as shown previously.

```{r, echo = F, fig.width=8, fig.height=4}
miles_stl <- FHWA_Data %>% 
  dplyr::filter(series == "B_Miles_Driven") %>% 
  dplyr::mutate(series = "Billions of Miles Driven, US") %>% 
  autoSTL(series_name = "Billions of Miles Driven, US", 365/12, 12, 13, T)
```

### Additional Resources

```{r, echo = FALSE, fig.width=10, fig.height=10}
# Credit to ChatGPT for the corrplot formatting
data_vif_adj %>% 
dplyr::select(-prediction) %>% 
cor() %>% 
corrplot( 
         method = "color",
         col = colorRampPalette(c("blue", "white", "red"))(200),
         type = "upper",
         order = "hclust",
         addCoef.col = "black",
         tl.col = "black",
         tl.srt = 45,
         diag = FALSE
)
```

```{r, Developer Notes, echo = FALSE}
# A place for the developer to note potential features and future directions to take the model.

# Can today's crack spread be used to predict tomorrows crude price? Perhaps is the crack spread is favorable enough we can expect a bullish run on crude as companies seek to increase production to capitalize on higher margins.

# Note the move to v2 API call on EIA

# 3 potential options for modelling future price: 
  # Regression model, and STI forecasts of dependent variables.
  # Regression model making predictions using lagged variables.
  # Regression model that predicts today price, and the difference between the prediction and the actual       # price is used.
  #The third option can be improved with Jarque-Bera distribution on the error terms, to determine how "off" a particular day is.

# Option 3 is preferable, since it can give us a band of expected prices today, and help us get a better sense of a strong/weak reversal potential.
```
